"""
í†µí•© ê²€ìƒ‰ ì—ì´ì „íŠ¸ (SearchAgent)
- ë‚´ë¶€ Knowledge Base ê²€ìƒ‰ + ì™¸ë¶€ Perplexity ê²€ìƒ‰ í†µí•©
- S3 ë©”íƒ€ë°ì´í„° í™œìš©í•œ ë‚ ì§œ ê¸°ë°˜ í•„í„°ë§
- ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ë° ì»¤ë²„ë¦¬ì§€ ë¶„ì„
- ì¡°ê±´ë¶€ ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰
"""
import json
import os
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import boto3
import re
from dataclasses import dataclass

logger = logging.getLogger()
logger.setLevel(logging.INFO)

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    sources: List[Dict]
    search_type: str
    coverage_score: float
    confidence: float
    timestamp: str
    metadata: Dict

@dataclass
class CombinedSearchResult:
    """í†µí•© ê²€ìƒ‰ ê²°ê³¼"""
    internal_result: Optional[SearchResult]
    external_result: Optional[SearchResult]
    combined_coverage: float
    recommended_sources: List[Dict]
    search_strategy_used: str
    execution_time: float
    success: bool = True  # ê²€ìƒ‰ ì„±ê³µ ì—¬ë¶€
    metadata: Dict = None  # ë©”íƒ€ë°ì´í„° ì†ì„± ì¶”ê°€
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ metadata ê¸°ë³¸ê°’ ì„¤ì •"""
        if self.metadata is None:
            self.metadata = {
                "search_strategy": self.search_strategy_used,
                "execution_time": self.execution_time,
                "sources_count": len(self.recommended_sources),
                "combined_coverage": self.combined_coverage
            }
    
    @property
    def sources(self) -> List[Dict]:
        """í˜¸í™˜ì„±ì„ ìœ„í•œ sources ì†ì„±"""
        return self.recommended_sources

class SearchAgent:
    """
    ë‚´ë¶€/ì™¸ë¶€ ê²€ìƒ‰ì„ í†µí•© ì²˜ë¦¬í•˜ëŠ” ê²€ìƒ‰ ì—ì´ì „íŠ¸
    """
    
    def __init__(self):
        # AWS í´ë¼ì´ì–¸íŠ¸ë“¤
        self.bedrock_agent_client = boto3.client("bedrock-agent-runtime", region_name="ap-northeast-2")
        self.s3_client = boto3.client("s3", region_name="ap-northeast-2")
        
        # ì„¤ì •ê°’
        self.knowledge_base_id = os.environ.get("KNOWLEDGE_BASE_ID", "PGQV3JXPET")
        self.s3_bucket = os.environ.get("NEWS_BUCKET", "seoul-news-data")
        
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            "internal_sufficient": 0.8,    # ë‚´ë¶€ ê²€ìƒ‰ë§Œìœ¼ë¡œ ì¶©ë¶„í•œ ê¸°ì¤€
            "external_trigger": 0.5,       # ì™¸ë¶€ ê²€ìƒ‰ í•„ìš” ê¸°ì¤€
            "coverage_minimum": 0.7,       # ìµœì†Œ ì»¤ë²„ë¦¬ì§€ ìš”êµ¬ì‚¬í•­
            "freshness_requirement": 0.8,  # ì‹ ì„ ë„ ìš”êµ¬ ì‹œ ì™¸ë¶€ ê²€ìƒ‰
            "max_sources": 10              # ìµœëŒ€ ì†ŒìŠ¤ ê°œìˆ˜
        }
        
        # S3 ë©”íƒ€ë°ì´í„° ì„¤ì •
        self.s3_date_format = "%Y-%m-%dT%H:%M:%S.%f%z"  # 2025-07-02T00:00:00.000+09:00
        
        # ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” import)
        self.external_search_agent = None
        self._initialize_external_agent()
        
        # ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ ê¸°ì¤€
        self.quality_criteria = {
            "relevance_keywords": ["ë‰´ìŠ¤", "ë³´ë„", "ë°œí‘œ", "ë¶„ì„", "ì „ë§"],
            "reliability_domains": ["sedaily.com", "yonhapnews.co.kr", "chosun.com"],
            "freshness_indicators": ["ìµœê·¼", "ì˜¤ëŠ˜", "ì–´ì œ", "ì´ë²ˆì£¼"]
        }
    
    def _initialize_external_agent(self):
        """ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        try:
            # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì ì ˆí•œ import ê²½ë¡œ ì‚¬ìš©
            from external_search.perplexity_integration import PerplexitySearchAgent
            self.external_search_agent = PerplexitySearchAgent()
            logger.info("ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except ImportError:
            logger.warning("ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì„í¬íŠ¸ ì‹¤íŒ¨, Mock ì‚¬ìš©")
            self.external_search_agent = self._create_mock_external_agent()
    
    def search_comprehensive(self, 
                            query: str,
                            search_strategy: str,
                            temporal_info: Dict = None,
                            context: Dict = None,
                            date_context: Dict = None) -> CombinedSearchResult:
        """
        í†µí•© ê²€ìƒ‰ ë©”ì¸ í•¨ìˆ˜ (ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"í†µí•© ê²€ìƒ‰ ì‹œì‘: {query} (ì „ëµ: {search_strategy})")
            
            # ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš© ë¡œê·¸
            if date_context:
                logger.info(f"ğŸ“… ê²€ìƒ‰ì— ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©: {date_context['í˜„ì¬_ë‚ ì§œ_ë¬¸ìì—´']}")
            
            # 1. ë‚´ë¶€ ê²€ìƒ‰ ì‹¤í–‰ (ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
            internal_result = self._execute_internal_search(query, temporal_info, context, date_context)
            
            # 2. ì»¤ë²„ë¦¬ì§€ í‰ê°€
            coverage_assessment = self._assess_coverage(internal_result, context)
            
            # 3. ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨
            needs_external = self._should_execute_external_search(
                coverage_assessment, search_strategy, temporal_info
            )
            
            external_result = None
            if needs_external:
                # 4. ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰
                external_result = self._execute_external_search(query, temporal_info, context)
            
            # 5. ê²°ê³¼ í†µí•© ë° ìµœì í™”
            combined_result = self._combine_and_optimize_results(
                internal_result, external_result, search_strategy
            )
            
            execution_time = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"ê²€ìƒ‰ ì™„ë£Œ - ë‚´ë¶€: {internal_result.coverage_score:.2f}, "
                       f"ì™¸ë¶€: {'Y' if external_result else 'N'}, ì‹œê°„: {execution_time:.2f}s")
            
            return CombinedSearchResult(
                internal_result=internal_result,
                external_result=external_result,
                combined_coverage=combined_result["coverage"],
                recommended_sources=combined_result["sources"],
                search_strategy_used=search_strategy,
                execution_time=execution_time
            )
            
        except Exception as e:
            logger.error(f"í†µí•© ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_fallback_search_result(query)
    
    def _execute_internal_search(self, 
                                query: str, 
                                temporal_info: Dict = None, 
                                context: Dict = None,
                                date_context: Dict = None) -> SearchResult:
        """
        ë‚´ë¶€ Knowledge Base ê²€ìƒ‰ ì‹¤í–‰
        """
        try:
            # S3 ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë‚ ì§œ í•„í„°ë§ ì¤€ë¹„ (ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì „ë‹¬)
            date_filter = self._prepare_date_filter(
                date_range=temporal_info, 
                date_context=date_context
            )
            
            # Knowledge Base ê²€ìƒ‰ ìš”ì²­ êµ¬ì„±
            search_request = {
                "knowledgeBaseId": self.knowledge_base_id,
                "retrievalQuery": {
                    "text": query
                },
                "retrievalConfiguration": {
                    "vectorSearchConfiguration": {
                        "numberOfResults": 20,  # ì¶©ë¶„í•œ ê²°ê³¼ í™•ë³´
                        "overrideSearchType": "HYBRID"  # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                    }
                }
            }
            
            # ë‚ ì§œ í•„í„° ì ìš© (ë©”íƒ€ë°ì´í„° ê¸°ë°˜)
            if date_filter:
                search_request["retrievalConfiguration"]["vectorSearchConfiguration"]["filter"] = date_filter
            
            # Bedrock Knowledge Base ê²€ìƒ‰ ì‹¤í–‰
            response = self.bedrock_agent_client.retrieve(**search_request)
            
            # ê²°ê³¼ ì²˜ë¦¬ ë° í’ˆì§ˆ í‰ê°€
            processed_result = self._process_internal_results(response, query, temporal_info)
            
            return processed_result
            
        except Exception as e:
            logger.error(f"ë‚´ë¶€ ê²€ìƒ‰ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return self._get_empty_search_result("internal", f"ë‚´ë¶€ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
    
    def _prepare_date_filter(self, date_range: Dict = None, date_context: Dict = None, date_context_manager: Dict = None) -> Dict:
        """
        Knowledge Base ê²€ìƒ‰ìš© ë‚ ì§œ í•„í„° ì¤€ë¹„
        DateContextManager í™œìš© (ì‚¬ìš©ì ì œì•ˆ êµ¬í˜„)
        """
        if not date_range and not date_context and not date_context_manager:
            return None
        
        try:
            # DateContextManager í™œìš© (ì‚¬ìš©ì ì œì•ˆ)
            if date_context_manager:
                logger.info(f"ğŸ“… DateContextManagerë¥¼ í™œìš©í•œ ë‚ ì§œ í•„í„° ìƒì„±")
                # ê¸°ë³¸ ë²”ìœ„: ìµœê·¼ 30ì¼
                end_date = date_context_manager['í˜„ì¬_ì‹œê°„']
                start_date = end_date - timedelta(days=30)
                logger.info(f"ê¸°ë³¸ ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
            elif date_range:
                # ê¸°ì¡´ date_range ì‚¬ìš©
                try:
                    start_date = datetime.fromisoformat(date_range["start_date"].replace('Z', '+00:00'))
                    end_date = datetime.fromisoformat(date_range["end_date"].replace('Z', '+00:00'))
                    logger.info(f"ì œê³µëœ ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
                except (KeyError, ValueError) as e:
                    logger.warning(f"ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {str(e)}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=30)
            else:
                # ìµœì¢… ëŒ€ì•ˆ
                end_date = datetime.now()
                start_date = end_date - timedelta(days=30)
                logger.info(f"ëŒ€ì•ˆ ë‚ ì§œ ë²”ìœ„: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}")
        
            # S3 ë©”íƒ€ë°ì´í„° í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (2025-07-02T00:00:00.000+09:00)
            start_str = start_date.strftime("%Y-%m-%dT%H:%M:%S.000+09:00")
            end_str = end_date.strftime("%Y-%m-%dT%H:%M:%S.000+09:00")
            
            # Knowledge Base ë©”íƒ€ë°ì´í„° í•„í„° êµ¬ì„±
            date_filter = {
                "andAll": [
                    {
                        "greaterThanOrEquals": {
                            "key": "published_date",
                            "value": start_str
                        }
                    },
                    {
                        "lessThanOrEquals": {
                            "key": "published_date", 
                            "value": end_str
                        }
                    }
                ]
            }
            
            logger.info(f"âœ… ë‚ ì§œ í•„í„° ìƒì„± ì™„ë£Œ: {start_str} ~ {end_str}")
            return date_filter
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ í•„í„° ìƒì„± ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _process_internal_results(self, 
                                 bedrock_response: Dict, 
                                 query: str, 
                                 temporal_info: Dict = None) -> SearchResult:
        """
        ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬
        """
        try:
            retrieval_results = bedrock_response.get("retrievalResults", [])
            
            if not retrieval_results:
                return self._get_empty_search_result("internal", "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
            
            # ê²°ê³¼ ì •ë¦¬ ë° ì ìˆ˜ ê³„ì‚°
            processed_sources = []
            total_relevance = 0.0
            content_pieces = []
            
            for i, result in enumerate(retrieval_results[:self.thresholds["max_sources"]]):
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ë°œí–‰ì¼ ì •ë³´ ì¶”ì¶œ (ë‰´ìŠ¤ ì„œë¹„ìŠ¤ í•„ìˆ˜)
                metadata = result.get("metadata", {})
                published_date_raw = metadata.get("published_date", "")
                
                # S3 ë°œí–‰ì¼ íŒŒì‹± (2025-07-02T00:00:00.000+09:00 í˜•ì‹)
                published_date_korean = self._format_published_date(published_date_raw)
                
                # ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ (ë°œí–‰ì¼ ì •ë³´ í¬í•¨)
                source_info = {
                    "index": i + 1,
                    "content": result.get("content", {}).get("text", ""),
                    "score": result.get("score", 0.0),
                    "metadata": metadata,
                    "uri": result.get("location", {}).get("s3Location", {}).get("uri", ""),
                    "relevance": self._calculate_relevance(result.get("content", {}).get("text", ""), query),
                    "published_date_raw": published_date_raw,  # ì›ë³¸ ë°œí–‰ì¼
                    "published_date_korean": published_date_korean,  # í•œêµ­ì–´ í˜•ì‹ ë°œí–‰ì¼
                    "has_date_info": bool(published_date_raw)  # ë‚ ì§œ ì •ë³´ ì¡´ì¬ ì—¬ë¶€
                }
                
                processed_sources.append(source_info)
                total_relevance += source_info["relevance"]
                
                # ë‚´ìš© ìˆ˜ì§‘ (ìš”ì•½ìš©)
                if source_info["content"]:
                    content_pieces.append(source_info["content"][:300] + "...")
            
            # ì „ì²´ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
            coverage_score = self._calculate_internal_coverage(processed_sources, query, temporal_info)
            
            # ê²°í•©ëœ ë‚´ìš© ìƒì„±
            combined_content = "\n\n".join(content_pieces[:5])  # ìƒìœ„ 5ê°œë§Œ
            
            return SearchResult(
                content=combined_content,
                sources=processed_sources,
                search_type="internal",
                coverage_score=coverage_score,
                confidence=min(total_relevance / len(processed_sources) if processed_sources else 0.0, 1.0),
                timestamp=datetime.now().isoformat(),
                metadata={
                    "total_results": len(retrieval_results),
                    "processed_results": len(processed_sources),
                    "avg_score": sum(s["score"] for s in processed_sources) / len(processed_sources) if processed_sources else 0.0,
                    "has_date_filter": temporal_info is not None and temporal_info.get("has_time_expression", False)
                }
            )
            
        except Exception as e:
            logger.error(f"ë‚´ë¶€ ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return self._get_empty_search_result("internal", f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    def _calculate_relevance(self, content: str, query: str) -> float:
        """
        ì»¨í…ì¸ ì™€ ì¿¼ë¦¬ ê°„ì˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        """
        if not content or not query:
            return 0.0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
        query_keywords = query.lower().split()
        content_lower = content.lower()
        
        keyword_matches = sum(1 for keyword in query_keywords if keyword in content_lower)
        keyword_score = keyword_matches / len(query_keywords) if query_keywords else 0.0
        
        # í’ˆì§ˆ ì§€í‘œ ì ìˆ˜
        quality_score = 0.0
        for indicator in self.quality_criteria["relevance_keywords"]:
            if indicator in content_lower:
                quality_score += 0.1
        
        # ìµœì¢… ì ìˆ˜ (0-1)
        final_score = (keyword_score * 0.7) + (min(quality_score, 0.3))
        return min(final_score, 1.0)
    
    def _calculate_internal_coverage(self, 
                                   sources: List[Dict], 
                                   query: str, 
                                   temporal_info: Dict = None) -> float:
        """
        ë‚´ë¶€ ê²€ìƒ‰ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        """
        if not sources:
            return 0.0
        
        base_coverage = 0.3  # ê¸°ë³¸ ì ìˆ˜
        
        # ì†ŒìŠ¤ ê°œìˆ˜ í‰ê°€
        if len(sources) >= 5:
            base_coverage += 0.2
        elif len(sources) >= 3:
            base_coverage += 0.1
        
        # ê´€ë ¨ì„± í‰ê°€
        avg_relevance = sum(s["relevance"] for s in sources) / len(sources)
        base_coverage += avg_relevance * 0.3
        
        # ì‹ ì„ ë„ í‰ê°€
        if temporal_info and temporal_info.get("freshness_priority", 0.0) > 0.7:
            # ìµœì‹  ì •ë³´ ìš”êµ¬ ì‹œ ì»¤ë²„ë¦¬ì§€ ê°ì†Œ (ì™¸ë¶€ ê²€ìƒ‰ ìœ ë„)
            base_coverage *= 0.7
        
        # ë‚ ì§œ í•„í„°ë§ ì ìš© ì‹œ ë³´ì •
        if temporal_info and temporal_info.get("has_time_expression"):
            base_coverage += 0.1  # ì •í™•í•œ ë‚ ì§œ ë²”ìœ„ ê²€ìƒ‰ì— ëŒ€í•œ ë³´ë„ˆìŠ¤
        
        return min(base_coverage, 1.0)
    
    def _assess_coverage(self, internal_result: SearchResult, context: Dict = None) -> Dict:
        """
        ê²€ìƒ‰ ì»¤ë²„ë¦¬ì§€ í‰ê°€
        """
        assessment = {
            "internal_coverage": internal_result.coverage_score,
            "confidence": internal_result.confidence,
            "source_count": len(internal_result.sources),
            "content_quality": self._assess_content_quality(internal_result.content),
            "recommendation": "sufficient"  # ê¸°ë³¸ê°’
        }
        
        # ì¶”ì²œ ê²°ì •
        if internal_result.coverage_score < self.thresholds["external_trigger"]:
            assessment["recommendation"] = "needs_external"
        elif internal_result.coverage_score < self.thresholds["internal_sufficient"]:
            assessment["recommendation"] = "consider_external"
        
        # ë§¥ë½ ê¸°ë°˜ ì¡°ì •
        if context:
            freshness_priority = context.get("freshness_priority", 0.0)
            if freshness_priority > self.thresholds["freshness_requirement"]:
                assessment["recommendation"] = "needs_external"
                assessment["reason"] = "high_freshness_requirement"
        
        return assessment
    
    def _assess_content_quality(self, content: str) -> float:
        """
        ì»¨í…ì¸  í’ˆì§ˆ í‰ê°€
        """
        if not content:
            return 0.0
        
        quality_score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ í‰ê°€
        if len(content) > 200:
            quality_score += 0.1
        if len(content) > 500:
            quality_score += 0.1
        
        # êµ¬ì¡° í‰ê°€
        if "." in content:
            quality_score += 0.1  # ë¬¸ì¥ êµ¬ì¡°
        if any(keyword in content for keyword in self.quality_criteria["relevance_keywords"]):
            quality_score += 0.2  # ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ
        
        return min(quality_score, 1.0)
    
    def _should_execute_external_search(self, 
                                      coverage_assessment: Dict,
                                      search_strategy: str, 
                                      temporal_info: Dict = None) -> bool:
        """
        ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
        """
        # ì»¤ë²„ë¦¬ì§€ ê¸°ë°˜ íŒë‹¨
        if coverage_assessment["recommendation"] == "needs_external":
            return True
        
        # ì „ëµ ê¸°ë°˜ íŒë‹¨
        if search_strategy in ["fresh_content_priority", "multi_source_search"]:
            return True
        
        # ì‹ ì„ ë„ ìš”êµ¬ì‚¬í•­ ê¸°ë°˜ íŒë‹¨
        if temporal_info:
            freshness_priority = temporal_info.get("freshness_priority", 0.0)
            if freshness_priority > self.thresholds["freshness_requirement"]:
                return True
            
            # ë‚ ì§œ í‘œí˜„ì´ ì—†ëŠ” ê²½ìš° (ìµœì‹ ìˆœ ìš”êµ¬)
            if temporal_info.get("search_mode") == "latest_first":
                return True
        
        return False
    
    def _execute_external_search(self, 
                                query: str, 
                                temporal_info: Dict = None, 
                                context: Dict = None) -> Optional[SearchResult]:
        """
        ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰
        """
        try:
            if not self.external_search_agent:
                logger.warning("ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸ ì—†ìŒ")
                return None
            
            # ì™¸ë¶€ ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            search_context = {
                "internal_coverage": context.get("internal_coverage", 0.0) if context else 0.0,
                "freshness_priority": temporal_info.get("freshness_priority", 0.0) if temporal_info else 0.0,
                "complexity_level": context.get("complexity_level", "simple") if context else "simple",
                "date_strategy": temporal_info or {}
            }
            
            # Perplexity ê²€ìƒ‰ ì‹¤í–‰
            external_result = self.external_search_agent.search_external_knowledge(
                query, search_context
            )
            
            # SearchResult í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return SearchResult(
                content=external_result.content,
                sources=[
                    {
                        "index": i + 1,
                        "title": source.get("title", ""),
                        "url": source.get("url", ""),
                        "snippet": source.get("snippet", ""),
                        "domain": source.get("domain", ""),
                        "relevance": 0.8  # ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ëŠ” ê¸°ë³¸ ë†’ì€ ê´€ë ¨ì„±
                    }
                    for i, source in enumerate(external_result.sources)
                ],
                search_type="external",
                coverage_score=external_result.confidence,
                confidence=external_result.confidence,
                timestamp=external_result.timestamp,
                metadata={
                    "token_usage": external_result.token_usage,
                    "search_provider": "perplexity",
                    "query_optimized": True
                }
            )
            
        except Exception as e:
            logger.error(f"ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}")
            return None
    
    def _combine_and_optimize_results(self, 
                                    internal_result: SearchResult,
                                    external_result: Optional[SearchResult], 
                                    search_strategy: str) -> Dict:
        """
        ë‚´ë¶€/ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ í†µí•© ë° ìµœì í™”
        """
        combined_sources = []
        combined_coverage = internal_result.coverage_score
        
        # ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
        for source in internal_result.sources:
            source["source_type"] = "internal"
            combined_sources.append(source)
        
        # ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if external_result:
            for source in external_result.sources:
                source["source_type"] = "external"
                combined_sources.append(source)
            
            # ì»¤ë²„ë¦¬ì§€ ê²°í•© (ê°€ì¤‘ í‰ê· )
            combined_coverage = (internal_result.coverage_score * 0.6 + 
                               external_result.coverage_score * 0.4)
        
        # ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì •ë ¬
        optimized_sources = self._prioritize_sources(combined_sources, search_strategy)
        
        return {
            "sources": optimized_sources[:self.thresholds["max_sources"]],
            "coverage": combined_coverage,
            "source_mix": {
                "internal_count": len([s for s in optimized_sources if s.get("source_type") == "internal"]),
                "external_count": len([s for s in optimized_sources if s.get("source_type") == "external"])
            }
        }
    
    def _prioritize_sources(self, sources: List[Dict], search_strategy: str) -> List[Dict]:
        """
        ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ì •ë ¬
        """
        def get_priority_score(source):
            score = source.get("relevance", 0.0)
            
            # ì „ëµë³„ ê°€ì¤‘ì¹˜ ì ìš©
            if search_strategy == "latest_first_search":
                if source.get("source_type") == "external":
                    score *= 1.2  # ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ ìš°ì„ 
            elif search_strategy == "date_filtered_search":
                if source.get("source_type") == "internal":
                    score *= 1.1  # ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼ ìš°ì„  (ì •í™•í•œ ë‚ ì§œ í•„í„°ë§)
            
            # ì‹ ë¢°ë„ ë†’ì€ ë„ë©”ì¸ ê°€ì¤‘ì¹˜
            if source.get("domain"):
                for trusted_domain in self.quality_criteria["reliability_domains"]:
                    if trusted_domain in source.get("domain", ""):
                        score *= 1.15
                        break
            
            return score
        
        return sorted(sources, key=get_priority_score, reverse=True)
    
    def _get_empty_search_result(self, search_type: str, reason: str) -> SearchResult:
        """ë¹ˆ ê²€ìƒ‰ ê²°ê³¼ ìƒì„±"""
        return SearchResult(
            content=f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {reason}",
            sources=[],
            search_type=search_type,
            coverage_score=0.0,
            confidence=0.0,
            timestamp=datetime.now().isoformat(),
            metadata={"error": reason}
        )
    
    def _get_fallback_search_result(self, query: str) -> CombinedSearchResult:
        """Fallback ê²€ìƒ‰ ê²°ê³¼"""
        fallback_internal = SearchResult(
            content=f"{query}ì— ëŒ€í•œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            sources=[],
            search_type="fallback",
            coverage_score=0.0,
            confidence=0.0,
            timestamp=datetime.now().isoformat(),
            metadata={"fallback": True}
        )
        
        return CombinedSearchResult(
            internal_result=fallback_internal,
            external_result=None,
            combined_coverage=0.0,
            recommended_sources=[],
            search_strategy_used="fallback",
            execution_time=0.0
        )
    
    def _create_mock_external_agent(self):
        """Mock ì™¸ë¶€ ê²€ìƒ‰ ì—ì´ì „íŠ¸"""
        class MockExternalAgent:
            def search_external_knowledge(self, query, context):
                from types import SimpleNamespace
                return SimpleNamespace(
                    content=f"Mock ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼: {query}",
                    sources=[{"title": "Mock Source", "url": "http://example.com"}],
                    confidence=0.7,
                    timestamp=datetime.now().isoformat(),
                    token_usage=100
                )
        return MockExternalAgent()
    
    def to_dict(self, combined_result: CombinedSearchResult) -> Dict:
        """CombinedSearchResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "internal_result": {
                "content": combined_result.internal_result.content if combined_result.internal_result else "",
                "sources": combined_result.internal_result.sources if combined_result.internal_result else [],
                "coverage_score": combined_result.internal_result.coverage_score if combined_result.internal_result else 0.0,
                "confidence": combined_result.internal_result.confidence if combined_result.internal_result else 0.0
            },
            "external_result": {
                "content": combined_result.external_result.content if combined_result.external_result else "",
                "sources": combined_result.external_result.sources if combined_result.external_result else [],
                "coverage_score": combined_result.external_result.coverage_score if combined_result.external_result else 0.0,
                "confidence": combined_result.external_result.confidence if combined_result.external_result else 0.0
            } if combined_result.external_result else None,
            "combined_coverage": combined_result.combined_coverage,
            "recommended_sources": combined_result.recommended_sources,
            "search_strategy_used": combined_result.search_strategy_used,
            "execution_time": combined_result.execution_time
        }

    def _format_published_date(self, published_date_raw: str) -> str:
        """
        S3 ë©”íƒ€ë°ì´í„°ì˜ ë°œí–‰ì¼ì„ í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        ì…ë ¥: "2025-07-02T00:00:00.000+09:00"
        ì¶œë ¥: "2025ë…„ 7ì›” 2ì¼ ì˜¤ì „ 12ì‹œ"
        """
        if not published_date_raw:
            return "ë°œí–‰ì¼ ë¯¸ìƒ"
        
        try:
            from datetime import datetime
            import pytz
            
            # ISO í˜•ì‹ íŒŒì‹±
            dt = datetime.fromisoformat(published_date_raw.replace('Z', '+00:00'))
            
            # í•œêµ­ ì‹œê°„ëŒ€ë¡œ ë³€í™˜
            kst = pytz.timezone('Asia/Seoul')
            if dt.tzinfo is None:
                dt = kst.localize(dt)
            else:
                dt = dt.astimezone(kst)
            
            # í•œêµ­ì–´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            year = dt.year
            month = dt.month
            day = dt.day
            hour = dt.hour
            minute = dt.minute
            
            # ì˜¤ì „/ì˜¤í›„ êµ¬ë¶„
            if hour == 0:
                time_str = "ì˜¤ì „ 12ì‹œ"
            elif hour < 12:
                time_str = f"ì˜¤ì „ {hour}ì‹œ"
            elif hour == 12:
                time_str = "ì˜¤í›„ 12ì‹œ"
            else:
                time_str = f"ì˜¤í›„ {hour-12}ì‹œ"
            
            # ë¶„ ì •ë³´ ì¶”ê°€ (0ë¶„ì´ ì•„ë‹Œ ê²½ìš°)
            if minute > 0:
                time_str += f" {minute}ë¶„"
            
            korean_date = f"{year}ë…„ {month}ì›” {day}ì¼ {time_str}"
            
            logger.info(f"ğŸ“… ë°œí–‰ì¼ ë³€í™˜: {published_date_raw} â†’ {korean_date}")
            return korean_date
            
        except Exception as e:
            logger.error(f"ë°œí–‰ì¼ ë³€í™˜ ì˜¤ë¥˜: {str(e)} - ì›ë³¸: {published_date_raw}")
            return f"ë°œí–‰ì¼ íŒŒì‹± ì˜¤ë¥˜ ({published_date_raw[:10]})"

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    search_agent = SearchAgent()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_cases = [
        {
            "query": "ì‚¼ì–‘ì‹í’ˆ ì£¼ê°€ ë™í–¥",
            "strategy": "latest_first_search",
            "temporal_info": {
                "has_time_expression": False,
                "search_mode": "latest_first",
                "freshness_priority": 0.8
            }
        },
        {
            "query": "1ë…„ ì „ ì‚¼ì„±ì „ì ì‹¤ì ",
            "strategy": "date_filtered_search", 
            "temporal_info": {
                "has_time_expression": True,
                "search_mode": "date_filtered",
                "calculated_date_range": {
                    "start_date": "2023-01-01T00:00:00+09:00",
                    "end_date": "2023-12-31T23:59:59+09:00"
                }
            }
        },
        {
            "query": "ìµœì‹  ê²½ì œ ë™í–¥",
            "strategy": "multi_source_search",
            "temporal_info": {
                "has_time_expression": False,
                "freshness_priority": 0.9
            }
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*80}")
        print(f"í…ŒìŠ¤íŠ¸ {i}: {test_case['query']} ({test_case['strategy']})")
        print('='*80)
        
        result = search_agent.search_comprehensive(
            test_case["query"],
            test_case["strategy"],
            test_case.get("temporal_info"),
            {"category": "ê²½ì œ"}
        )
        
        result_dict = search_agent.to_dict(result)
        print(json.dumps(result_dict, ensure_ascii=False, indent=2))
    
    print(f"\n{'='*80}")
    print("SearchAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print('='*80) 