"""
í†µí•© ë¶„ì„ ì—ì´ì „íŠ¸ (AnalyzerAgent)
- ì§ˆë¬¸ ëª…í™•ì„± í‰ê°€ + ë‚ ì§œ ê°ì§€ í†µí•©
- ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ë° ì—”í‹°í‹° ì¶”ì¶œ
- ê²€ìƒ‰ ì „ëµ ê²°ì •
- ìŠ¬ë¦¼í™”ëœ ë‹¨ì¼ ì—ì´ì „íŠ¸ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
"""
import json
import re
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import boto3
from dataclasses import dataclass
import pytz

logger = logging.getLogger()
logger.setLevel(logging.INFO)

@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼ ë°ì´í„° í´ë˜ìŠ¤"""
    clarity_score: float
    category: str
    entities: Dict
    temporal_info: Dict
    search_strategy: str
    confidence: float
    needs_clarification: bool
    recommended_actions: List[str]

class AnalyzerAgent:
    """
    ì§ˆë¬¸ ë¶„ì„ê³¼ ë‚ ì§œ ì²˜ë¦¬ë¥¼ í†µí•©í•œ ìŠ¬ë¦¼í™”ëœ ì—ì´ì „íŠ¸
    (ê¸°ì¡´ Query Analyzer + Time Calculator í†µí•©)
    """
    
    def __init__(self):
        # í•œêµ­ ì‹œê°„ëŒ€
        self.kst = pytz.timezone('Asia/Seoul')
        self.current_time = datetime.now(self.kst)
        
        # Bedrock í´ë¼ì´ì–¸íŠ¸
        self.bedrock_client = boto3.client("bedrock-runtime", region_name="ap-northeast-2")
        
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            "clarity_minimum": 0.8,      # ì¬ì§ˆë¬¸ í•„ìš” ê¸°ì¤€
            "freshness_high": 0.7,       # ì™¸ë¶€ ê²€ìƒ‰ í•„ìš” ê¸°ì¤€
            "confidence_minimum": 0.75   # ì „ì²´ ì‹ ë¢°ë„ ê¸°ì¤€
        }
        
        # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ
        self.category_keywords = {
            "ê²½ì œ": ["ê²½ì œ", "ê¸ˆìœµ", "ì¦ì‹œ", "ì£¼ê°€", "ì‹¤ì ", "ë§¤ì¶œ", "ìˆ˜ìµ", "íˆ¬ì", "í€ë“œ", "ì€í–‰"],
            "ê¸°ì—…": ["ê¸°ì—…", "íšŒì‚¬", "CEO", "ëŒ€í‘œ", "ì‚¬ì—…", "ê²½ì˜", "ì¸ìˆ˜", "í•©ë³‘", "ìƒì¥", "IPO"],
            "ì •ì¹˜": ["ì •ë¶€", "ì •ì±…", "ë²•ì•ˆ", "ì •ì¹˜", "êµ­íšŒ", "ëŒ€í†µë ¹", "ì¥ê´€", "ì„ ê±°", "ì—¬ë‹¹", "ì•¼ë‹¹"],
            "ê¸°ìˆ ": ["ê¸°ìˆ ", "IT", "í˜ì‹ ", "ê°œë°œ", "ë””ì§€í„¸", "AI", "ì¸ê³µì§€ëŠ¥", "ì†Œí”„íŠ¸ì›¨ì–´", "í•˜ë“œì›¨ì–´"],
            "ì‚¬íšŒ": ["ì‚¬íšŒ", "êµìœ¡", "ì˜ë£Œ", "ë³µì§€", "ë¬¸í™”", "ìŠ¤í¬ì¸ ", "ì—°ì˜ˆ", "ì‚¬ê±´", "ì‚¬ê³ "],
            "êµ­ì œ": ["í•´ì™¸", "ë¯¸êµ­", "ì¤‘êµ­", "ì¼ë³¸", "ìœ ëŸ½", "ë¬´ì—­", "ì™¸êµ", "êµ­ì œ", "ê¸€ë¡œë²Œ"]
        }
        
        # ì‹œê°„ í‘œí˜„ íŒ¨í„´ (ê°„ì†Œí™”ëœ ë²„ì „)
        self.time_patterns = {
            "relative": {
                r'ì˜¤ëŠ˜|í˜„ì¬|ì§€ê¸ˆ': 0,
                r'ì–´ì œ': -1,
                r'ìµœê·¼|ìš”ì¦˜': -7,
                r'ì‘ë…„|ì§€ë‚œí•´': -365,
                r'(\d+)ë…„\s*ì „': lambda m: -int(m.group(1)) * 365,
                r'(\d+)ê°œ?ì›”\s*ì „': lambda m: -int(m.group(1)) * 30,
                r'(\d+)ì¼\s*ì „': lambda m: -int(m.group(1))
            },
            "absolute": [
                r'(\d{4})ë…„',
                r'(\d{1,2})ì›”',
                r'ìƒë°˜ê¸°|í•˜ë°˜ê¸°|1ë¶„ê¸°|2ë¶„ê¸°|3ë¶„ê¸°|4ë¶„ê¸°'
            ],
            "freshness": [
                r'ìµœì‹ |ì‹¤ì‹œê°„|ì†ë³´|ê¸´ê¸‰|ë¼ì´ë¸Œ'
            ]
        }
        
        # ì£¼ìš” ê¸°ì—…ëª… íŒ¨í„´
        self.company_patterns = [
            r'ì‚¼ì„±[ì „ì|SDI|ë°”ì´ì˜¤ë¡œì§ìŠ¤|í™”ì¬|ë¬¼ì‚°]*',
            r'LG[ì „ì|í™”í•™|ì—ë„ˆì§€ì†”ë£¨ì…˜|ë””ìŠ¤í”Œë ˆì´]*',
            r'í˜„ëŒ€[ìë™ì°¨|ëª¨í„°|ì¤‘ê³µì—…|ê±´ì„¤]*',
            r'SK[í•˜ì´ë‹‰ìŠ¤|í…”ë ˆì½¤|ì´ë…¸ë² ì´ì…˜|ë°”ì´ì˜¤íŒœ]*',
            r'í¬ìŠ¤ì½”|POSCO',
            r'ë„¤ì´ë²„|NAVER',
            r'ì¹´ì¹´ì˜¤|Kakao',
            r'ë°°ë‹¬ì˜ë¯¼ì¡±|ì¿ íŒ¡|ë§ˆì¼“ì»¬ë¦¬'
        ]
    
    def analyze_query(self, query: str, context: Dict = None, date_context: Dict = None) -> AnalysisResult:
        """
        í†µí•© ì§ˆë¬¸ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜
        """
        try:
            logger.info(f"ì§ˆë¬¸ ë¶„ì„ ì‹œì‘: {query}")
            
            # ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš© (ì‚¬ìš©ì ì œì•ˆ êµ¬í˜„)
            if date_context:
                logger.info(f"ğŸ“… ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©: {date_context['í˜„ì¬_ë‚ ì§œ_ë¬¸ìì—´']}")
                # í˜„ì¬ ì‹œê°„ì„ date_contextì—ì„œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
                self.current_time = date_context['í˜„ì¬_ì‹œê°„']
            
            # 1. ê¸°ë³¸ ë¶„ì„
            basic_metrics = self._calculate_basic_metrics(query)
            
            # 2. ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            category = self._classify_category(query)
            
            # 3. ì—”í‹°í‹° ì¶”ì¶œ
            entities = self._extract_entities(query)
            
            # 4. ì‹œê°„ ì •ë³´ ë¶„ì„ (ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
            temporal_info = self._analyze_temporal_expressions(query, date_context)
            
            # 5. ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°
            clarity_score = self._calculate_clarity_score(query, basic_metrics, entities)
            
            # 6. ê²€ìƒ‰ ì „ëµ ê²°ì •
            search_strategy = self._determine_search_strategy(temporal_info, category, clarity_score)
            
            # 7. ì¶”ì²œ ì•¡ì…˜ ìƒì„±
            recommended_actions = self._generate_recommendations(clarity_score, temporal_info, category)
            
            # 8. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_overall_confidence(clarity_score, entities, temporal_info)
            
            # 9. ì¬ì§ˆë¬¸ í•„ìš”ì„± íŒë‹¨
            needs_clarification = clarity_score < self.thresholds["clarity_minimum"]
            
            result = AnalysisResult(
                clarity_score=clarity_score,
                category=category,
                entities=entities,
                temporal_info=temporal_info,
                search_strategy=search_strategy,
                confidence=confidence,
                needs_clarification=needs_clarification,
                recommended_actions=recommended_actions
            )
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ - ì¹´í…Œê³ ë¦¬: {category}, ì „ëµ: {search_strategy}, ì‹ ë¢°ë„: {confidence:.2f}")
            return result
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_fallback_result(query)
    
    def _calculate_basic_metrics(self, query: str) -> Dict:
        """ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        return {
            "length": len(query),
            "word_count": len(query.split()),
            "has_question_mark": "?" in query,
            "has_korean": bool(re.search(r'[ê°€-í£]', query)),
            "has_numbers": bool(re.search(r'\d', query)),
            "has_english": bool(re.search(r'[a-zA-Z]', query))
        }
    
    def _classify_category(self, query: str) -> str:
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        category_scores = {}
        
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in query)
            if score > 0:
                category_scores[category] = score
        
        if category_scores:
            return max(category_scores, key=category_scores.get)
        
        # ê¸°ë³¸ ë¶„ë¥˜ ë¡œì§
        if any(word in query for word in ["ì£¼ê°€", "íˆ¬ì", "ì‹¤ì "]):
            return "ê²½ì œ"
        elif any(word in query for word in ["íšŒì‚¬", "ê¸°ì—…", "CEO"]):
            return "ê¸°ì—…"
        else:
            return "ì¼ë°˜"
    
    def _extract_entities(self, query: str) -> Dict:
        """ì£¼ìš” ì—”í‹°í‹° ì¶”ì¶œ"""
        entities = {
            "companies": [],
            "persons": [],
            "numbers": [],
            "keywords": []
        }
        
        # ê¸°ì—…ëª… ì¶”ì¶œ
        for pattern in self.company_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            entities["companies"].extend(matches)
        
        # ìˆ«ì ì¶”ì¶œ
        numbers = re.findall(r'\d+(?:\.\d+)?', query)
        entities["numbers"] = [float(n) if '.' in n else int(n) for n in numbers]
        
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)
        important_words = ["ì£¼ê°€", "ì‹¤ì ", "ë§¤ì¶œ", "ì´ìµ", "ì „ë§", "ê³„íš", "ë°œí‘œ", "ì¶œì‹œ"]
        entities["keywords"] = [word for word in important_words if word in query]
        
        return entities
    
    def _analyze_temporal_expressions(self, query: str, date_context: Dict = None) -> Dict:
        """ì‹œê°„ í‘œí˜„ ë¶„ì„ - í•µì‹¬ ê¸°ëŠ¥! (ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©)"""
        temporal_info = {
            "has_time_expression": False,
            "detected_expressions": [],
            "time_type": "none",
            "calculated_date_range": None,
            "freshness_priority": 0.0,
            "search_mode": "latest_first"  # ê¸°ë³¸ê°’
        }
        
        try:
            # ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í™œìš© (ì‚¬ìš©ì ì œì•ˆ êµ¬í˜„)
            if date_context:
                logger.info(f"ğŸ“… ì‹œê°„ ë¶„ì„ì— ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ í™œìš©: {date_context['í˜„ì¬_ë…„ë„']}ë…„ ê¸°ì¤€")
                
                # DateContextManagerì˜ ê³„ì‚° ê¸°ëŠ¥ í™œìš©
                from utils.date_context_manager import get_date_context_manager
                date_manager = get_date_context_manager()
                
                # ë‚ ì§œ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸
                if date_manager.is_date_related_query(query):
                    temporal_info["has_time_expression"] = True
                    logger.info(f"ğŸ• ë‚ ì§œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€: {query}")
            
            # 1. ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ê°ì§€
            relative_detected = []
            for pattern, offset in self.time_patterns["relative"].items():
                matches = re.finditer(pattern, query, re.IGNORECASE)
                for match in matches:
                    if callable(offset):
                        days_offset = offset(match)
                    else:
                        days_offset = offset
                    
                    # ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì •í™•í•œ ê³„ì‚°
                    if date_context and date_manager:
                        calculated_date = date_manager.calculate_relative_date(match.group())
                        logger.info(f"âœ… '{match.group()}' â†’ {calculated_date['year']}ë…„ (ì •í™•í•œ ê³„ì‚°)")
                        
                        relative_detected.append({
                            "expression": match.group(),
                            "days_offset": days_offset,
                            "calculated_year": calculated_date['year'],
                            "calculated_date": calculated_date['date_string'],
                            "type": "relative"
                        })
                    else:
                        relative_detected.append({
                            "expression": match.group(),
                            "days_offset": days_offset,
                            "type": "relative"
                        })
            
            # 2. ì ˆëŒ€ì  ì‹œê°„ í‘œí˜„ ê°ì§€
            absolute_detected = []
            for pattern in self.time_patterns["absolute"]:
                matches = re.finditer(pattern, query, re.IGNORECASE)
                for match in matches:
                    absolute_detected.append({
                        "expression": match.group(),
                        "type": "absolute"
                    })
            
            # 3. ì‹ ì„ ë„ í‚¤ì›Œë“œ ê°ì§€
            freshness_score = 0.0
            for pattern in self.time_patterns["freshness"]:
                if re.search(pattern, query, re.IGNORECASE):
                    freshness_score += 0.3
            
            temporal_info["freshness_priority"] = min(freshness_score, 1.0)
            
            # 4. ê°ì§€ëœ í‘œí˜„ë“¤ ì¢…í•©
            all_detected = relative_detected + absolute_detected
            
            if all_detected:
                temporal_info["has_time_expression"] = True
                temporal_info["detected_expressions"] = all_detected
                
                # ê°€ì¥ êµ¬ì²´ì ì¸ ì‹œê°„ í‘œí˜„ ì„ íƒ
                if relative_detected:
                    primary_expression = relative_detected[0]
                    temporal_info["time_type"] = "relative"
                    temporal_info["calculated_date_range"] = self._calculate_date_range(primary_expression)
                    temporal_info["search_mode"] = "date_filtered"
                elif absolute_detected:
                    temporal_info["time_type"] = "absolute"
                    temporal_info["search_mode"] = "date_filtered"
            
            # 5. ì‹œê°„ í‘œí˜„ì´ ì—†ìœ¼ë©´ ìµœì‹ ìˆœ ìš°ì„ 
            if not temporal_info["has_time_expression"]:
                temporal_info["calculated_date_range"] = {
                    "start_date": (self.current_time - timedelta(days=30)).isoformat(),
                    "end_date": self.current_time.isoformat(),
                    "priority": "latest_first",
                    "reason": "no_date_expression_default_to_recent"
                }
                temporal_info["search_mode"] = "latest_first"
            
            return temporal_info
            
        except Exception as e:
            logger.error(f"ì‹œê°„ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                "has_time_expression": False,
                "search_mode": "latest_first",
                "calculated_date_range": {
                    "start_date": (self.current_time - timedelta(days=7)).isoformat(),
                    "end_date": self.current_time.isoformat(),
                    "priority": "latest_first",
                    "reason": "fallback_recent"
                }
            }
    
    def _calculate_date_range(self, time_expression: Dict) -> Dict:
        """ì‹œê°„ í‘œí˜„ì„ ì‹¤ì œ ë‚ ì§œ ë²”ìœ„ë¡œ ë³€í™˜"""
        try:
            days_offset = time_expression["days_offset"]
            target_date = self.current_time + timedelta(days=days_offset)
            
            if days_offset == 0:  # ì˜¤ëŠ˜
                return {
                    "start_date": target_date.replace(hour=0, minute=0, second=0).isoformat(),
                    "end_date": target_date.replace(hour=23, minute=59, second=59).isoformat(),
                    "priority": "date_specific",
                    "reason": "today"
                }
            elif days_offset == -1:  # ì–´ì œ
                return {
                    "start_date": target_date.replace(hour=0, minute=0, second=0).isoformat(),
                    "end_date": target_date.replace(hour=23, minute=59, second=59).isoformat(),
                    "priority": "date_specific",
                    "reason": "yesterday"
                }
            elif days_offset <= -30:  # í•œ ë‹¬ ì´ìƒ ì „
                return {
                    "start_date": (target_date - timedelta(days=30)).isoformat(),
                    "end_date": (target_date + timedelta(days=30)).isoformat(),
                    "priority": "date_range",
                    "reason": f"around_{abs(days_offset)}_days_ago"
                }
            else:  # ìµœê·¼ (7ì¼ ì „ ë“±)
                return {
                    "start_date": (target_date - timedelta(days=3)).isoformat(),
                    "end_date": (target_date + timedelta(days=3)).isoformat(),
                    "priority": "recent_range",
                    "reason": "recent_period"
                }
                
        except Exception as e:
            logger.error(f"ë‚ ì§œ ë²”ìœ„ ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
            return {
                "start_date": (self.current_time - timedelta(days=7)).isoformat(),
                "end_date": self.current_time.isoformat(),
                "priority": "latest_first",
                "reason": "calculation_error_fallback"
            }
    
    def _calculate_clarity_score(self, query: str, basic_metrics: Dict, entities: Dict) -> float:
        """ì§ˆë¬¸ ëª…í™•ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸¸ì´ í‰ê°€
        if basic_metrics["length"] > 10:
            score += 0.1
        if basic_metrics["word_count"] >= 3:
            score += 0.1
        
        # ì§ˆë¬¸ í˜•íƒœ í‰ê°€
        if basic_metrics["has_question_mark"]:
            score += 0.15
        
        # êµ¬ì²´ì„± í‰ê°€
        if entities["companies"]:
            score += 0.15  # êµ¬ì²´ì ì¸ ê¸°ì—…ëª… ìˆìŒ
        if entities["keywords"]:
            score += 0.1   # ê´€ë ¨ í‚¤ì›Œë“œ ìˆìŒ
        
        # ë¶€ì • ìš”ì†Œ
        if basic_metrics["length"] < 5:
            score -= 0.3
        if not basic_metrics["has_korean"] and not basic_metrics["has_english"]:
            score -= 0.2
        
        # ì• ë§¤í•œ í‘œí˜„ ê°ì§€
        vague_patterns = [r'^[ê°€-í£]{1,3}\?*$', r'^[a-zA-Z]{1,5}\?*$']
        if any(re.match(pattern, query) for pattern in vague_patterns):
            score -= 0.4
        
        return max(0.0, min(1.0, score))
    
    def _determine_search_strategy(self, temporal_info: Dict, category: str, clarity_score: float) -> str:
        """ê²€ìƒ‰ ì „ëµ ê²°ì •"""
        
        # ëª…í™•ì„±ì´ ë‚®ìœ¼ë©´ ì¬ì§ˆë¬¸ ìš°ì„ 
        if clarity_score < self.thresholds["clarity_minimum"]:
            return "clarification_first"
        
        # ì‹œê°„ í‘œí˜„ì´ ìˆìœ¼ë©´ ë‚ ì§œ ê¸°ë°˜ ê²€ìƒ‰
        if temporal_info["has_time_expression"]:
            return "date_filtered_search"
        
        # ì‹ ì„ ë„ê°€ ë†’ê²Œ ìš”êµ¬ë˜ë©´ ì™¸ë¶€ ê²€ìƒ‰ í¬í•¨
        if temporal_info["freshness_priority"] > self.thresholds["freshness_high"]:
            return "fresh_content_priority"
        
        # ë³µì¡í•œ ì¹´í…Œê³ ë¦¬ëŠ” ë‹¤ë‹¨ê³„ ê²€ìƒ‰
        if category in ["ê²½ì œ", "ì •ì¹˜"]:
            return "multi_source_search"
        
        # ê¸°ë³¸ê°’: ìµœì‹ ìˆœ ê²€ìƒ‰
        return "latest_first_search"
    
    def _generate_recommendations(self, clarity_score: float, temporal_info: Dict, category: str) -> List[str]:
        """ì¶”ì²œ ì•¡ì…˜ ìƒì„±"""
        actions = []
        
        # í•„ìˆ˜: ë‚´ë¶€ ê²€ìƒ‰
        actions.append("internal_search")
        
        # ì¡°ê±´ë¶€: ì¬ì§ˆë¬¸
        if clarity_score < self.thresholds["clarity_minimum"]:
            actions.append("query_clarification")
        
        # ì¡°ê±´ë¶€: ì™¸ë¶€ ê²€ìƒ‰
        if (temporal_info["freshness_priority"] > self.thresholds["freshness_high"] or
            not temporal_info["has_time_expression"]):
            actions.append("external_search")
        
        # ì¡°ê±´ë¶€: ë‚ ì§œ í•„í„°ë§
        if temporal_info["has_time_expression"]:
            actions.append("date_filtering")
        
        # í•„ìˆ˜: ë‹µë³€ ìƒì„±
        actions.append("answer_synthesis")
        
        return actions
    
    def _calculate_overall_confidence(self, clarity_score: float, entities: Dict, temporal_info: Dict) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        confidence = clarity_score * 0.4  # ëª…í™•ì„± 40%
        
        # ì—”í‹°í‹° ì¡´ì¬ë¡œ ì‹ ë¢°ë„ ì¦ê°€
        if entities["companies"]:
            confidence += 0.2
        if entities["keywords"]:
            confidence += 0.1
        
        # ì‹œê°„ ì •ë³´ë¡œ ì‹ ë¢°ë„ ì¦ê°€
        if temporal_info["has_time_expression"]:
            confidence += 0.15
        else:
            confidence += 0.1  # ìµœì‹ ìˆœ ê²€ìƒ‰ë„ ìœ íš¨
        
        # ì‹ ì„ ë„ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ì¡°ì •
        freshness_bonus = temporal_info["freshness_priority"] * 0.15
        confidence += freshness_bonus
        
        return min(confidence, 1.0)
    
    def _get_fallback_result(self, query: str) -> AnalysisResult:
        """Fallback ë¶„ì„ ê²°ê³¼"""
        return AnalysisResult(
            clarity_score=0.6,
            category="ì¼ë°˜",
            entities={"companies": [], "persons": [], "numbers": [], "keywords": []},
            temporal_info={
                "has_time_expression": False,
                "search_mode": "latest_first",
                "calculated_date_range": {
                    "start_date": (self.current_time - timedelta(days=7)).isoformat(),
                    "end_date": self.current_time.isoformat(),
                    "priority": "latest_first",
                    "reason": "fallback"
                }
            },
            search_strategy="latest_first_search",
            confidence=0.5,
            needs_clarification=False,
            recommended_actions=["internal_search", "answer_synthesis"]
        )
    
    def generate_clarification_questions(self, query: str, analysis_result: AnalysisResult) -> List[str]:
        """ì¬ì§ˆë¬¸ ìƒì„±"""
        questions = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ êµ¬ì²´í™” ì§ˆë¬¸
        if analysis_result.category == "ê²½ì œ":
            questions.extend([
                "êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ê²½ì œ ì§€í‘œì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?",
                "íŠ¹ì • ì‹œê¸°ì˜ ê²½ì œ ìƒí™©ì„ ì›í•˜ì‹œë‚˜ìš”?",
                "ì „ë°˜ì ì¸ ê²½ì œ ë™í–¥ì„ ì›í•˜ì‹œë‚˜ìš”?"
            ])
        elif analysis_result.category == "ê¸°ì—…":
            questions.extend([
                "íŠ¹ì • ê¸°ì—…ì˜ ì–´ë–¤ ì¸¡ë©´ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”? (ì‹¤ì , ì£¼ê°€, ì‚¬ì—… ê³„íš ë“±)",
                "ìµœê·¼ ì†Œì‹ì„ ì›í•˜ì‹œë‚˜ìš”, ì•„ë‹ˆë©´ íŠ¹ì • ì‹œì ì˜ ì •ë³´ë¥¼ ì›í•˜ì‹œë‚˜ìš”?"
            ])
        
        # ì‹œê°„ ê´€ë ¨ êµ¬ì²´í™”
        if not analysis_result.temporal_info["has_time_expression"]:
            questions.append("ì–¸ì œ ì‹œì ì˜ ì •ë³´ë¥¼ ì›í•˜ì‹œë‚˜ìš”? (ìµœê·¼, íŠ¹ì • ë‚ ì§œ, ê¸°ê°„ ë“±)")
        
        # ì—”í‹°í‹° ê´€ë ¨ êµ¬ì²´í™”
        if not analysis_result.entities["companies"] and analysis_result.category in ["ê²½ì œ", "ê¸°ì—…"]:
            questions.append("íŠ¹ì • íšŒì‚¬ë‚˜ ê¸°ì—…ì— ëŒ€í•œ ì •ë³´ë¥¼ ì›í•˜ì‹œë‚˜ìš”?")
        
        return questions[:3]  # ìµœëŒ€ 3ê°œ
    
    def to_dict(self, analysis_result: AnalysisResult) -> Dict:
        """AnalysisResultë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "clarity_score": analysis_result.clarity_score,
            "category": analysis_result.category,
            "entities": analysis_result.entities,
            "temporal_info": analysis_result.temporal_info,
            "search_strategy": analysis_result.search_strategy,
            "confidence": analysis_result.confidence,
            "needs_clarification": analysis_result.needs_clarification,
            "recommended_actions": analysis_result.recommended_actions
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    analyzer = AnalyzerAgent()
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
    test_queries = [
        "ì‚¼ì–‘ì‹í’ˆ ì£¼ê°€ëŠ” ì–´ë–¤ê°€ìš”?",           # ëª…í™•, ë‚ ì§œ ì—†ìŒ â†’ ìµœì‹ ìˆœ
        "1ë…„ ì „ ì‚¼ì„±ì „ìëŠ” ì–´ë• ë‚˜ìš”?",         # ëª…í™•, ë‚ ì§œ ìˆìŒ â†’ ë‚ ì§œ í•„í„°ë§
        "ì–´ì œ ì£¼ìš” ë‰´ìŠ¤ëŠ”?",                  # ë³´í†µ, ë‚ ì§œ ìˆìŒ â†’ ë‚ ì§œ íŠ¹ì •
        "ë°˜ë„ì²´?",                           # ì• ë§¤ â†’ ì¬ì§ˆë¬¸ í•„ìš”
        "ìµœì‹  ê²½ì œ ë™í–¥ ë¶„ì„í•´ì¤˜",            # ì‹ ì„ ë„ ë†’ìŒ â†’ ì™¸ë¶€ ê²€ìƒ‰
        "ì‘ë…„ ìƒë°˜ê¸° ì‹¤ì "                    # ì ˆëŒ€ ë‚ ì§œ â†’ ë‚ ì§œ í•„í„°ë§
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n{'='*60}")
        print(f"í…ŒìŠ¤íŠ¸ {i}: {query}")
        print('='*60)
        
        result = analyzer.analyze_query(query)
        result_dict = analyzer.to_dict(result)
        
        print(json.dumps(result_dict, ensure_ascii=False, indent=2))
        
        # ì¬ì§ˆë¬¸ì´ í•„ìš”í•œ ê²½ìš°
        if result.needs_clarification:
            questions = analyzer.generate_clarification_questions(query, result)
            print(f"\nì¬ì§ˆë¬¸ í›„ë³´:")
            for j, q in enumerate(questions, 1):
                print(f"  {j}. {q}")
    
    print(f"\n{'='*60}")
    print("AnalyzerAgent í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print('='*60) 