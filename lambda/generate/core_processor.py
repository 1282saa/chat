"""
ì„œìš¸ê²½ì œì‹ ë¬¸ AI ìš”ì•½ ì‹œìŠ¤í…œ - í•µì‹¬ ì²˜ë¦¬ ì—”ì§„
ê³µí†µ ë¡œì§ ì¤‘ì•™í™”ë¡œ ì¤‘ë³µ ì œê±° ë° ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
"""

import json
import boto3
import os
import logging
import traceback
from datetime import datetime
import sys
from pathlib import Path

# ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent / 'date_intelligence'))
sys.path.append(str(Path(__file__).parent.parent / 'external_search'))
sys.path.append(str(Path(__file__).parent.parent / 'utils'))

try:
    from date_processor import DateIntelligenceProcessor
    from perplexity_integration import PerplexitySearchAgent
    from common_utils import DecimalEncoder
    from apac_model_manager import APACModelManager
except ImportError as e:
    logger.error(f"ëª¨ë“ˆ import ì˜¤ë¥˜: {e}")

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# AWS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì„œìš¸ë¦¬ì „)
bedrock_runtime = boto3.client(
    service_name='bedrock-runtime',
    region_name=os.environ.get('AWS_REGION', 'ap-northeast-2')
)

bedrock_agent_runtime = boto3.client(
    service_name='bedrock-agent-runtime', 
    region_name=os.environ.get('AWS_REGION', 'ap-northeast-2')
)

s3_client = boto3.client('s3', region_name=os.environ.get('AWS_REGION', 'ap-northeast-2'))

# í™˜ê²½ ë³€ìˆ˜ - CDK ìŠ¤íƒê³¼ ë™ì¼í•œ ê°’ë“¤
KNOWLEDGE_BASE_ID = os.environ.get('KNOWLEDGE_BASE_ID', 'PGQV3JXPET')
S3_BUCKET_NAME = os.environ.get('S3_BUCKET_NAME', 'seoul-economic-news-data-2025')
NEWS_BUCKET = os.environ.get('NEWS_BUCKET', 'seoul-economic-news-data-2025')
PERPLEXITY_API_KEY = os.environ.get('PERPLEXITY_API_KEY', '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')
AWS_REGION = os.environ.get('AWS_REGION', 'ap-northeast-2')

# APAC ëª¨ë¸ ì§€ì› - CDKì—ì„œ í…ŒìŠ¤íŠ¸ëœ ì‹¤ì œ ëª¨ë¸ë“¤
SUPPORTED_CLAUDE_MODELS = {
    "claude-3-haiku": "apac.anthropic.claude-3-haiku-20240307-v1:0",
    "claude-3-sonnet": "apac.anthropic.claude-3-sonnet-20240229-v1:0", 
    "claude-3.5-sonnet": "apac.anthropic.claude-3-5-sonnet-20240620-v1:0",
    "claude-3.5-sonnet-v2": "apac.anthropic.claude-3-5-sonnet-20241022-v2:0",
    "claude-3.7-sonnet": "apac.anthropic.claude-3-7-sonnet-20250219-v1:0",
    "claude-4": "apac.anthropic.claude-sonnet-4-20250514-v1:0"
}

DEFAULT_MODEL_ID = "apac.anthropic.claude-3-5-sonnet-20240620-v1:0"

class NewsProcessor:
    """ì„œìš¸ê²½ì œì‹ ë¬¸ ë‰´ìŠ¤ ì²˜ë¦¬ í•µì‹¬ ì—”ì§„"""
    
    def __init__(self):
        """ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”© ë°©ì‹)"""
        self.date_processor = None
        self.perplexity_searcher = None
        self.model_manager = None
        
    def get_model_id(self, requested_model=None):
        """ìš”ì²­ëœ ëª¨ë¸ ID ë°˜í™˜ (ì„œìš¸ë¦¬ì „ APAC ëª¨ë¸ ì§€ì›)"""
        if not requested_model:
            return DEFAULT_MODEL_ID
            
        # ì§ì ‘ APAC ëª¨ë¸ IDê°€ ì „ë‹¬ëœ ê²½ìš°
        if requested_model.startswith("apac.anthropic.claude"):
            return requested_model
            
        # ê°„ë‹¨í•œ ëª¨ë¸ëª…ìœ¼ë¡œ ì „ë‹¬ëœ ê²½ìš°
        return SUPPORTED_CLAUDE_MODELS.get(requested_model, DEFAULT_MODEL_ID)
    
    def test_knowledge_base_connection(self):
        """Knowledge Base ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if not KNOWLEDGE_BASE_ID:
                return {
                    'success': False,
                    'error': 'KNOWLEDGE_BASE_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'
                }
            
            logger.info(f"ğŸ” Knowledge Base ì—°ê²° í…ŒìŠ¤íŠ¸: {KNOWLEDGE_BASE_ID}")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì‹¤í–‰
            test_query = "ì„œìš¸ê²½ì œì‹ ë¬¸ í…ŒìŠ¤íŠ¸"
            response = bedrock_agent_runtime.retrieve(
                knowledgeBaseId=KNOWLEDGE_BASE_ID,
                retrievalQuery={'text': test_query},
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': 5
                    }
                }
            )
            
            results = response.get('retrievalResults', [])
            logger.info(f"âœ… Knowledge Base ì—°ê²° ì„±ê³µ! í…ŒìŠ¤íŠ¸ ê²°ê³¼: {len(results)}ê°œ")
            
            return {
                'success': True,
                'knowledge_base_id': KNOWLEDGE_BASE_ID,
                'test_results_count': len(results),
                'region': AWS_REGION
            }
            
        except Exception as e:
            logger.error(f"âŒ Knowledge Base ì—°ê²° ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'knowledge_base_id': KNOWLEDGE_BASE_ID
            }
    
    def test_claude_model(self, model_tier="claude-3.5-sonnet"):
        """Claude ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            model_id = self.get_model_id(model_tier)
            logger.info(f"ğŸ¤– Claude ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_id}")
            
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
            test_prompt = "ì•ˆë…•í•˜ì„¸ìš”, í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."
            
            # ëª¨ë¸ë³„ ìš”ì²­ í˜•ì‹
            if "claude-3" in model_id:
                body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 1000,
                    "messages": [
                        {
                            "role": "user",
                            "content": test_prompt
                        }
                    ]
                }
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ í˜•ì‹ (í•„ìš”ì‹œ)
                body = {
                    "prompt": f"\n\nHuman: {test_prompt}\n\nAssistant:",
                    "max_tokens_to_sample": 100
                }
            
            response = bedrock_runtime.invoke_model(
                modelId=model_id,
                body=json.dumps(body)
            )
            
            response_body = json.loads(response['body'].read())
            
            if "claude-3" in model_id:
                result_text = response_body['content'][0]['text']
            else:
                result_text = response_body.get('completion', 'Unknown response format')
            
            logger.info(f"âœ… Claude ëª¨ë¸ ì—°ê²° ì„±ê³µ! ì‘ë‹µ: {result_text[:50]}...")
            
            return {
                'success': True,
                'model_id': model_id,
                'model_tier': model_tier,
                'test_response': result_text,
                'region': AWS_REGION
            }
            
        except Exception as e:
            logger.error(f"âŒ Claude ëª¨ë¸ ì—°ê²° ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'model_id': model_id,
                'model_tier': model_tier
            }

    def enhance_query_with_date(self, user_input):
        """1ë‹¨ê³„: ë‚ ì§œ ì •ì˜ ë° ì§ˆë¬¸ ë³´ê°•"""
        try:
            # ì˜¤ëŠ˜ ë‚ ì§œ ì •ì˜
            today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
            
            # ë‚ ì§œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
            if not self.date_processor:
                self.date_processor = DateIntelligenceProcessor()
            
            # ìì—°ì–´ ë‚ ì§œ í‘œí˜„ ì²˜ë¦¬
            processed_query = self.date_processor.analyze_query_temporal_expressions(user_input)
            
            # ì§ˆë¬¸ ë³´ê°• (ì˜¤ëŠ˜ ë‚ ì§œ ì¶”ê°€)
            enhanced_query = f"ì˜¤ëŠ˜ì€ {today}ì…ë‹ˆë‹¤. {user_input}"
            
            logger.info(f"ğŸ“… ë‚ ì§œ ë³´ê°• ì™„ë£Œ: {enhanced_query}")
            return enhanced_query
            
        except Exception as e:
            logger.error(f"âŒ ë‚ ì§œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            # ê¸°ë³¸ ë‚ ì§œ ë³´ê°•
            today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
            return f"ì˜¤ëŠ˜ì€ {today}ì…ë‹ˆë‹¤. {user_input}"

    def search_knowledge_base(self, enhanced_query):
        """2ë‹¨ê³„: AWS ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰ (ìµœì‹ ìˆœ)"""
        try:
            if not KNOWLEDGE_BASE_ID:
                logger.warning("Knowledge Base IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return "ë‚´ë¶€ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            logger.info(f"ğŸ” Knowledge Base ê²€ìƒ‰ ì‹œì‘: {enhanced_query[:100]}...")
            
            # Bedrock Knowledge Base ê²€ìƒ‰ (ìµœì‹ ìˆœ ì •ë ¬)
            response = bedrock_agent_runtime.retrieve(
                knowledgeBaseId=KNOWLEDGE_BASE_ID,
                retrievalQuery={'text': enhanced_query},
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': 20  # ì¶©ë¶„í•œ ê²°ê³¼ ìˆ˜
                    }
                }
            )
            
            results = response.get('retrievalResults', [])
            
            if not results:
                logger.warning("Knowledge Baseì—ì„œ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return "ê´€ë ¨ ë‰´ìŠ¤ ìë£Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ë¥¼ ë°œí–‰ì¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì‹ ìˆœ ì •ë ¬
            sorted_results = sorted(results, 
                key=lambda x: x.get('metadata', {}).get('publish_date', ''), 
                reverse=True
            )
            
            # ìƒìœ„ ê²°ê³¼ë“¤ ê²°í•©
            knowledge_text = ""
            for idx, result in enumerate(sorted_results[:10], 1):
                content = result.get('content', {}).get('text', '')
                metadata = result.get('metadata', {})
                publish_date = metadata.get('publish_date', 'Unknown')
                
                knowledge_text += f"\n[ìë£Œ {idx}] ({publish_date})\n{content}\n"
            
            logger.info(f"âœ… Knowledge Base ê²€ìƒ‰ ì™„ë£Œ: {len(sorted_results)}ê±´")
            return knowledge_text
            
        except Exception as e:
            logger.error(f"âŒ Knowledge Base ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return "ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def should_use_external_search(self, knowledge_context, user_input):
        """ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨"""
        try:
            # ë‚´ë¶€ ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•œ ê²½ìš°
            if len(knowledge_context) < 200:
                return True
            
            # ìµœì‹  ì •ë³´ê°€ í•„ìš”í•œ í‚¤ì›Œë“œë“¤
            recent_keywords = ['ì‹¤ì‹œê°„', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ìµœì‹ ', 'ë°©ê¸ˆ', 'ì§€ê¸ˆ']
            if any(keyword in user_input for keyword in recent_keywords):
                return True
            
            # íŠ¹ì • ì—…ì¢…/í…Œë§ˆ ê´€ë ¨
            specific_keywords = ['ì£¼ê°€', 'ì‹¤ì ', 'ì •ì±…', 'ê·œì œ', 'ë°œí‘œ', 'ë‰´ìŠ¤']
            if any(keyword in user_input for keyword in specific_keywords):
                return True
                
            return False
            
        except Exception as e:
            logger.error(f"ì™¸ë¶€ ê²€ìƒ‰ íŒë‹¨ ì˜¤ë¥˜: {e}")
            return False

    def search_external_knowledge(self, enhanced_query):
        """3ë‹¨ê³„: Perplexity APIë¡œ ì™¸ë¶€ ì§€ì‹ ë³´ê°•"""
        try:
            logger.info(f"ğŸŒ ì™¸ë¶€ ê²€ìƒ‰ ì‹œì‘: {enhanced_query[:100]}...")
            
            # Perplexity ê²€ìƒ‰ê¸° ì´ˆê¸°í™” (ì§€ì—° ë¡œë”©)
            if not self.perplexity_searcher:
                self.perplexity_searcher = PerplexitySearchAgent()
            
            # ê²€ìƒ‰ ì‹¤í–‰
            search_result = self.perplexity_searcher.search_external_knowledge(enhanced_query, {})
            
            if search_result and hasattr(search_result, 'content') and search_result.content:
                logger.info(f"âœ… ì™¸ë¶€ ê²€ìƒ‰ ì™„ë£Œ: {len(search_result.content)}ì")
                return search_result.content
            else:
                logger.warning("ì™¸ë¶€ ê²€ìƒ‰ì—ì„œ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return "ì™¸ë¶€ ê²€ìƒ‰ì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                
        except Exception as e:
            logger.error(f"âŒ ì™¸ë¶€ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return "ì™¸ë¶€ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def build_final_prompt(self, user_input, chat_history, knowledge_context):
        """4ë‹¨ê³„: ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ë¡œ ì¶œë ¥êµ¬ì¡° íŒŒì•… ë° ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ë‚´ë¶€ í”„ë¡¬í”„íŠ¸ - ì„œìš¸ê²½ì œì‹ ë¬¸ AI ìš”ì•½ ì‹œìŠ¤í…œ ì „ìš©
        system_prompt = """ë‹¹ì‹ ì€ ì„œìš¸ê²½ì œì‹ ë¬¸ì˜ ì „ë¬¸ AI ê¸°ìì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë‰´ìŠ¤ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.

ë‹µë³€ êµ¬ì¡°:
1. í•µì‹¬ ìš”ì•½ (2-3ë¬¸ì¥)
2. ìƒì„¸ ë¶„ì„ (ê´€ë ¨ ë°ì´í„° ë° ë§¥ë½ í¬í•¨)
3. ì‹œì¥/ì‚¬íšŒì  ì˜í–¥
4. ì „ë§ ë° ì˜ê²¬

ë‹µë³€ ì›ì¹™:
- ì œê³µëœ ë‰´ìŠ¤ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±
- ì •í™•í•œ ìˆ˜ì¹˜ì™€ ë‚ ì§œ ì¸ìš©
- ê· í˜•ì¡íŒ ì‹œê°ìœ¼ë¡œ ë¶„ì„
- ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
- ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…ì‹œ
"""

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì²˜ë¦¬
        history_text = ""
        if chat_history:
            for msg in chat_history[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ
                role = msg.get('role', '')
                content = msg.get('content', '')
                if role == 'user':
                    history_text += f"ì‚¬ìš©ì: {content}\n"
                elif role == 'assistant':
                    history_text += f"AI: {content[:200]}...\n"
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        final_prompt = f"""{system_prompt}

=== ëŒ€í™” ë§¥ë½ ===
{history_text}

=== ê´€ë ¨ ë‰´ìŠ¤ ìë£Œ ===
{knowledge_context}

=== ì‚¬ìš©ì ì§ˆë¬¸ ===
{user_input}

=== AI ë‹µë³€ ===
"""
        
        logger.info("ğŸ“ ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì™„ë£Œ")
        return final_prompt

    def generate_with_bedrock(self, prompt, model_tier="claude-3.5-sonnet"):
        """5ë‹¨ê³„: AWS Bedrockìœ¼ë¡œ ìµœì¢… ìƒì„±"""
        try:
            model_id = self.get_model_id(model_tier)
            logger.info(f"ğŸ¤– Bedrock ìƒì„± ì‹œì‘: {model_id}")
            
            # Claude 3 ì‹œë¦¬ì¦ˆ ìš”ì²­ í˜•ì‹
            if "claude-3" in model_id:
                body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4000,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "top_p": 0.9
                }
            else:
                # ê¸°íƒ€ ëª¨ë¸ (í•„ìš”ì‹œ)
                body = {
                    "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
                    "max_tokens_to_sample": 4000,
                    "temperature": 0.3,
                    "top_p": 0.9
                }
            
            response = bedrock_runtime.invoke_model(
                modelId=model_id,
                body=json.dumps(body)
            )
            
            response_body = json.loads(response['body'].read())
            
            if "claude-3" in model_id:
                generated_text = response_body['content'][0]['text']
            else:
                generated_text = response_body.get('completion', 'Generation failed')
            
            logger.info(f"âœ… Bedrock ìƒì„± ì™„ë£Œ: {len(generated_text)}ì")
            return generated_text
            
        except Exception as e:
            logger.error(f"âŒ Bedrock ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def stream_bedrock_response(self, connection_id, prompt, model_tier="claude-3.5-sonnet", send_message_func=None):
        """ìŠ¤íŠ¸ë¦¬ë° ìƒì„± (WebSocketìš©)"""
        try:
            model_id = self.get_model_id(model_tier)
            logger.info(f"ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: {model_id}")
            
            if "claude-3" in model_id:
                body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": 4000,
                    "messages": [
                        {
                            "role": "user",
                            "content": prompt
                        }
                    ],
                    "temperature": 0.3,
                    "top_p": 0.9
                }
            else:
                body = {
                    "prompt": f"\n\nHuman: {prompt}\n\nAssistant:",
                    "max_tokens_to_sample": 4000,
                    "temperature": 0.3,
                    "top_p": 0.9
                }
            
            response = bedrock_runtime.invoke_model_with_response_stream(
                modelId=model_id,
                body=json.dumps(body)
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            full_response = ""
            for event in response['body']:
                chunk = json.loads(event['chunk']['bytes'])
                
                if "claude-3" in model_id:
                    if chunk.get('type') == 'content_block_delta':
                        delta_text = chunk.get('delta', {}).get('text', '')
                        if delta_text:
                            full_response += delta_text
                            if send_message_func:
                                send_message_func(connection_id, {
                                    'type': 'chunk',
                                    'content': delta_text
                                })
                else:
                    # ë‹¤ë¥¸ ëª¨ë¸ ì²˜ë¦¬
                    if 'completion' in chunk:
                        delta_text = chunk['completion']
                        full_response += delta_text
                        if send_message_func:
                            send_message_func(connection_id, {
                                'type': 'chunk', 
                                'content': delta_text
                            })
            
            # ì™„ë£Œ ë©”ì‹œì§€
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'complete',
                    'full_response': full_response
                })
            
            logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì™„ë£Œ: {len(full_response)}ì")
            return full_response
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì˜¤ë¥˜: {e}")
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'error',
                    'message': f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {str(e)}"
                })
            return f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def process_complete_flow(self, user_input, chat_history=[], model_tier="claude-3.5-sonnet"):
        """ì „ì²´ 7ë‹¨ê³„ í”Œë¡œìš° ì²˜ë¦¬ (REST APIìš©)"""
        try:
            logger.info("ğŸš€ ì „ì²´ í”Œë¡œìš° ì‹œì‘")
            
            # 1-2ë‹¨ê³„: ë‚ ì§œ ë³´ê°•
            enhanced_query = self.enhance_query_with_date(user_input)
            
            # 3-4ë‹¨ê³„: ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰
            knowledge_context = self.search_knowledge_base(enhanced_query)
            
            # 5ë‹¨ê³„: ì™¸ë¶€ ê²€ìƒ‰ í•„ìš”ì„± íŒë‹¨ ë° ì‹¤í–‰
            if self.should_use_external_search(knowledge_context, user_input):
                external_context = self.search_external_knowledge(enhanced_query)
                knowledge_context += f"\n\n=== ì™¸ë¶€ ì°¸ì¡° ìë£Œ ===\n{external_context}"
            
            # 6ë‹¨ê³„: ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            final_prompt = self.build_final_prompt(user_input, chat_history, knowledge_context)
            
            # 7ë‹¨ê³„: Bedrock ìƒì„±
            response_content = self.generate_with_bedrock(final_prompt, model_tier)
            
            logger.info("âœ… ì „ì²´ í”Œë¡œìš° ì™„ë£Œ")
            return response_content
            
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ í”Œë¡œìš° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

    def process_streaming_flow(self, connection_id, user_input, chat_history=[], model_tier="claude-3.5-sonnet", send_message_func=None):
        """ì „ì²´ 7ë‹¨ê³„ í”Œë¡œìš° ì²˜ë¦¬ (WebSocket ìŠ¤íŠ¸ë¦¬ë°ìš©)"""
        try:
            logger.info("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° ì‹œì‘")
            
            # ìƒíƒœ ë©”ì‹œì§€ ì „ì†¡
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'status',
                    'message': 'ğŸ“… ë‚ ì§œ ì •ë³´ ì²˜ë¦¬ ì¤‘...'
                })
            
            # 1-2ë‹¨ê³„: ë‚ ì§œ ë³´ê°•
            enhanced_query = self.enhance_query_with_date(user_input)
            
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'status',
                    'message': 'ğŸ” ë‚´ë¶€ ìë£Œ ê²€ìƒ‰ ì¤‘...'
                })
            
            # 3-4ë‹¨ê³„: ë‚´ë¶€ ì§€ì‹ ê²€ìƒ‰
            knowledge_context = self.search_knowledge_base(enhanced_query)
            
            # 5ë‹¨ê³„: ì™¸ë¶€ ê²€ìƒ‰
            if self.should_use_external_search(knowledge_context, user_input):
                if send_message_func:
                    send_message_func(connection_id, {
                        'type': 'status',
                        'message': 'ğŸŒ ì™¸ë¶€ ìë£Œ ê²€ìƒ‰ ì¤‘...'
                    })
                
                external_context = self.search_external_knowledge(enhanced_query)
                knowledge_context += f"\n\n=== ì™¸ë¶€ ì°¸ì¡° ìë£Œ ===\n{external_context}"
            
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'status',
                    'message': 'ğŸ“ ë‹µë³€ ìƒì„± ì¤‘...'
                })
            
            # 6ë‹¨ê³„: ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            final_prompt = self.build_final_prompt(user_input, chat_history, knowledge_context)
            
            # 7ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
            self.stream_bedrock_response(connection_id, final_prompt, model_tier, send_message_func)
            
            logger.info("âœ… ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° í”Œë¡œìš° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            if send_message_func:
                send_message_func(connection_id, {
                    'type': 'error',
                    'message': f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                })

# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_news_processor_instance = None

def get_news_processor():
    """NewsProcessor ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _news_processor_instance
    if _news_processor_instance is None:
        _news_processor_instance = NewsProcessor()
    return _news_processor_instance 