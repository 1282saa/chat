"""
AI ëŒ€í™” ìƒì„± Lambda í•¨ìˆ˜ (Enhanced Agent System)
- ìƒˆë¡œìš´ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í†µí•©
- ReAct + CoT Planning ì ìš©
- ë‚ ì§œ ì§€ëŠ¥í˜• ì²˜ë¦¬ ë° Perplexity ê²€ìƒ‰
- Few-shot ê¸°ë°˜ ë‹µë³€ ìƒì„±
- ì„ê³„ê°’ ê¸°ë°˜ ì¡°ê±´ë¶€ ì‹¤í–‰
"""
import json
import os
import traceback
import boto3
import logging
import re
import urllib.request
import urllib.parse
import urllib.error
from datetime import datetime
import sys

# Logger ì„¤ì • (ë¨¼ì € ì„¤ì •)
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ import
sys.path.append('/opt/python')  # Lambda Layer ê²½ë¡œ
sys.path.append('.')

# Smart Query Router import (ìµœìš°ì„ )
try:
    from smart_router.query_router import SmartQueryRouter
    logger.info("ğŸ¯ SmartQueryRouter ë¡œë“œ ì„±ê³µ")
    SMART_ROUTER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"SmartQueryRouter ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    SMART_ROUTER_AVAILABLE = False
except Exception as e:
    logger.error(f"SmartQueryRouter ì˜¤ë¥˜: {str(e)}")
    SMART_ROUTER_AVAILABLE = False

# Fallback: Enhanced Agent System
try:
    from workflow_engine.conditional_execution import ConditionalExecutionEngine
    logger.info("ConditionalExecutionEngine ë¡œë“œ ì„±ê³µ")
    ENHANCED_AGENTS_AVAILABLE = True
    logger.info("Enhanced Agent System Fallback ì¤€ë¹„ ì™„ë£Œ")
    
except ImportError as e:
    logger.warning(f"Enhanced Agent Systemë„ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
    logger.warning(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    ENHANCED_AGENTS_AVAILABLE = False
except Exception as e:
    logger.error(f"Enhanced Agent System ì˜¤ë¥˜: {str(e)}")
    logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
    ENHANCED_AGENTS_AVAILABLE = False

# --- AWS í´ë¼ì´ì–¸íŠ¸ ë° ê¸°ë³¸ ì„¤ì • ---
bedrock_client = boto3.client("bedrock-runtime", region_name=os.environ.get("REGION", "ap-northeast-2"))
bedrock_agent_client = boto3.client("bedrock-agent-runtime", region_name=os.environ.get("REGION", "ap-northeast-2"))
dynamodb_client = boto3.client("dynamodb", region_name=os.environ.get("REGION", "ap-northeast-2"))
PROMPT_META_TABLE = os.environ.get("PROMPT_META_TABLE", "ChatbotPrompts")
# Knowledge Base ì„¤ì •
KNOWLEDGE_BASE_ID = os.environ.get("KNOWLEDGE_BASE_ID", "PGQV3JXPET")
# Perplexity API ì„¤ì •
PERPLEXITY_API_KEY = os.environ.get("PERPLEXITY_API_KEY")
# ê¸°ë³¸ ëª¨ë¸ ID (í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì§€ì •í•˜ì§€ ì•Šì„ ë•Œ ì‚¬ìš©)
DEFAULT_MODEL_ID = "apac.anthropic.claude-3-haiku-20240307-v1:0"

# ë¡œê¹… ì„¤ì • (ìœ„ì—ì„œ ì´ë¯¸ ì„¤ì •ë¨)

# ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ (Inference Profile IDs)
SUPPORTED_MODELS = {
    # Anthropic Claude ëª¨ë¸ë“¤ (Inference Profiles)
    "apac.anthropic.claude-sonnet-4-20250514-v1:0": {"name": "Claude Sonnet 4", "provider": "Anthropic"},
    "apac.anthropic.claude-3-7-sonnet-20250219-v1:0": {"name": "Claude 3.7 Sonnet", "provider": "Anthropic"},
    "apac.anthropic.claude-3-5-sonnet-20241022-v2:0": {"name": "Claude 3.5 Sonnet v2", "provider": "Anthropic"},
    "apac.anthropic.claude-3-5-sonnet-20240620-v1:0": {"name": "Claude 3.5 Sonnet", "provider": "Anthropic"},
    "apac.anthropic.claude-3-haiku-20240307-v1:0": {"name": "Claude 3 Haiku", "provider": "Anthropic"},
    "apac.anthropic.claude-3-sonnet-20240229-v1:0": {"name": "Claude 3 Sonnet", "provider": "Anthropic"},
    
    # Legacy Direct Model IDs (fallback)
    "anthropic.claude-3-haiku-20240307-v1:0": {"name": "Claude 3 Haiku", "provider": "Anthropic"},
    "anthropic.claude-3-sonnet-20240229-v1:0": {"name": "Claude 3 Sonnet", "provider": "Anthropic"},
    
    # Meta Llama ëª¨ë¸ë“¤
    "meta.llama4-scout-17b-instruct-v4:0": {"name": "Llama 4 Scout 17B", "provider": "Meta"},
    "meta.llama4-maverick-17b-instruct-v4:0": {"name": "Llama 4 Maverick 17B", "provider": "Meta"},
    "meta.llama3-3-70b-instruct-v1:0": {"name": "Llama 3.3 70B", "provider": "Meta"},
    "meta.llama3-2-11b-instruct-v1:0": {"name": "Llama 3.2 11B Vision", "provider": "Meta"},
    "meta.llama3-2-1b-instruct-v1:0": {"name": "Llama 3.2 1B", "provider": "Meta"},
    "meta.llama3-2-3b-instruct-v1:0": {"name": "Llama 3.2 3B", "provider": "Meta"},
    
    # Amazon Nova ëª¨ë¸ë“¤
    "amazon.nova-premier-v1:0": {"name": "Nova Premier", "provider": "Amazon"},
    "amazon.nova-lite-v1:0": {"name": "Nova Lite", "provider": "Amazon"},
    "amazon.nova-micro-v1:0": {"name": "Nova Micro", "provider": "Amazon"},
    "amazon.nova-pro-v1:0": {"name": "Nova Pro", "provider": "Amazon"},
}

def handler(event, context):
    """
    API Gateway ìš”ì²­ì„ ì²˜ë¦¬í•˜ì—¬ Bedrock ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    - GET ìš”ì²­ì€ EventSource (SSE)ë¥¼ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤ (ê¸´ URL ë¬¸ì œë¡œ í˜„ì¬ëŠ” ë¹„ê¶Œì¥).
    - POST ìš”ì²­ì´ ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ì…ë‹ˆë‹¤.
    """
    try:
        logger.info("ğŸš€ Handler ì‹œì‘")
        logger.info(f"ğŸ” ì´ë²¤íŠ¸ ìˆ˜ì‹ : {json.dumps(event)}")
        logger.info(f"ğŸ” SmartRouter ì‚¬ìš© ê°€ëŠ¥: {SMART_ROUTER_AVAILABLE}")
        logger.info(f"ğŸ” Enhanced Agents ì‚¬ìš© ê°€ëŠ¥: {ENHANCED_AGENTS_AVAILABLE}")
        
        http_method = event.get("httpMethod", "POST")
        path = event.get("path", "")
        project_id = event.get("pathParameters", {}).get("projectId")
        
        logger.info(f"ğŸ” HTTP Method: {http_method}, Path: {path}, Project ID: {project_id}")

        # ğŸ¯ SmartQueryRouterëŠ” í”„ë¡œì íŠ¸ ID ì—†ì´ë„ ì‘ë™ ê°€ëŠ¥
        if not project_id:
            logger.info("âš¡ í”„ë¡œì íŠ¸ ID ì—†ìŒ - SmartQueryRouter ëª¨ë“œë¡œ ì‹¤í–‰")
            project_id = "default"  # ê¸°ë³¸ê°’ ì„¤ì •

        if not project_id:
            logger.error("âŒ í”„ë¡œì íŠ¸ ID ëˆ„ë½")
            return _create_error_response(400, "í”„ë¡œì íŠ¸ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìš”ì²­ ë³¸ë¬¸(body) íŒŒì‹±
        try:
            if http_method == 'GET':
                params = event.get('queryStringParameters') or {}
                user_input = params.get('userInput', '')
                chat_history_str = params.get('chat_history', '[]')
                chat_history = json.loads(chat_history_str)
                model_id = params.get('modelId', DEFAULT_MODEL_ID)
                use_knowledge_base = False
            else: # POST
                body = json.loads(event.get('body', '{}'))
                user_input = body.get('userInput', '')
                chat_history = body.get('chat_history', [])
                prompt_cards = body.get('prompt_cards', [])
                model_id = body.get('modelId', DEFAULT_MODEL_ID)
                use_knowledge_base = body.get('useKnowledgeBase', False)
                
            logger.info(f"ğŸ” íŒŒì‹± ì™„ë£Œ - user_input: {user_input[:50]}..., use_knowledge_base: {use_knowledge_base}")
                
        except Exception as parse_error:
            logger.error(f"âŒ ìš”ì²­ íŒŒì‹± ì˜¤ë¥˜: {str(parse_error)}")
            return _create_error_response(400, f"ìš”ì²­ íŒŒì‹± ì˜¤ë¥˜: {str(parse_error)}")
            
        if not user_input.strip():
            logger.error("âŒ ì‚¬ìš©ì ì…ë ¥ ëˆ„ë½")
            return _create_error_response(400, "ì‚¬ìš©ì ì…ë ¥ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ëª¨ë¸ ID ê²€ì¦
        if model_id not in SUPPORTED_MODELS:
            logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ ID: {model_id}")
            model_id = DEFAULT_MODEL_ID
        
        logger.info(f"ğŸ” ìµœì¢… ì„¤ì • - ëª¨ë¸: {model_id}, Knowledge Base: {use_knowledge_base}")
        
        # GET ìš”ì²­ì¼ ë•Œ prompt_cards ì²˜ë¦¬
        if http_method == 'GET':
            prompt_cards = []
        
        logger.info(f"âœ… ì„ íƒëœ ëª¨ë¸: {model_id} ({SUPPORTED_MODELS.get(model_id, {}).get('name', 'Unknown')})")
        
        # Knowledge Base ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë¶„ê¸°
        if use_knowledge_base:
            logger.info("ğŸ¯ Knowledge Base ì²˜ë¦¬ ì‹œì‘")
            return _handle_knowledge_base_generation(project_id, user_input, chat_history, model_id)
        
        # ìŠ¤íŠ¸ë¦¬ë° ë˜ëŠ” ì¼ë°˜ ìƒì„± ë¶„ê¸°
        if "/stream" in path:
            logger.info("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì‹œì‘")
            return _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards, model_id)
        else:
            logger.info("ğŸ“ ì¼ë°˜ ì²˜ë¦¬ ì‹œì‘")
            return _handle_standard_generation(project_id, user_input, chat_history, prompt_cards, model_id)

    except json.JSONDecodeError as json_error:
        logger.error(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {str(json_error)}")
        return _create_error_response(400, "ì˜ëª»ëœ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ Handler ì˜¤ë¥˜: {str(e)}")
        logger.error(f"âŒ Traceback: {traceback.format_exc()}")
        return _create_error_response(500, f"ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜: {e}")

def _handle_streaming_generation(project_id, user_input, chat_history, prompt_cards, model_id):
    """
    Bedrockì—ì„œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ë°›ì•„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì²­í¬ë³„ë¡œ ì¦‰ì‹œ SSE í˜•ì‹ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    try:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}, ëª¨ë¸={model_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        # ëª¨ë¸ì— ë”°ë¥¸ ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        if model_id.startswith("anthropic."):
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": final_prompt}],
                "temperature": 0.1,
                "top_p": 0.9,
            }
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ë“¤ì„ ìœ„í•œ ìš”ì²­ í˜•ì‹
            request_body = {
                "prompt": final_prompt,
                "max_gen_len": 4096,
                "temperature": 0.1,
                "top_p": 0.9,
            }

        response_stream = bedrock_client.invoke_model_with_response_stream(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        
        # ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ - ë²„í¼ë§ ìµœì†Œí™”
        sse_chunks = []
        full_response = ""
        
        # ì‹œì‘ ì´ë²¤íŠ¸
        start_data = {
            "response": "",
            "sessionId": project_id,
            "type": "start"
        }
        sse_chunks.append(f"data: {json.dumps(start_data)}\n\n")
        
        # ì‹¤ì‹œê°„ ì²­í¬ ì²˜ë¦¬ - ìµœì†Œ ì§€ì—°
        for event in response_stream.get("body"):
            chunk = json.loads(event["chunk"]["bytes"].decode())
            
            # ëª¨ë¸ë³„ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
            text = None
            if model_id.startswith("anthropic."):
                # Anthropic ëª¨ë¸ ì‘ë‹µ í˜•ì‹
                if chunk['type'] == 'content_block_delta':
                    text = chunk['delta']['text']
            else:
                # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ ì‘ë‹µ í˜•ì‹
                if 'generation' in chunk:
                    text = chunk['generation']
                elif 'text' in chunk:
                    text = chunk['text']
            
            if text:
                full_response += text
                
                # ì¦‰ì‹œ ì²­í¬ ì „ì†¡ (ë²„í¼ë§ ì—†ìŒ)
                sse_data = {
                    "response": text,
                    "sessionId": project_id,
                    "type": "chunk"
                }
                sse_chunks.append(f"data: {json.dumps(sse_data)}\n\n")
        
        # ì™„ë£Œ ì´ë²¤íŠ¸ ì „ì†¡
        completion_data = {
            "response": "",
            "sessionId": project_id,
            "type": "complete",
            "fullResponse": full_response
        }
        sse_chunks.append(f"data: {json.dumps(completion_data)}\n\n")
        
        print(f"ìŠ¤íŠ¸ë¦¬ë° ìƒì„± ì™„ë£Œ: ì´ {len(sse_chunks)} ì²­í¬ ìƒì„±ë¨, ì‘ë‹µ ê¸¸ì´={len(full_response)}")
        return {
            "statusCode": 200,
            "headers": _get_sse_headers(),
            "body": "".join(sse_chunks),
            "isBase64Encoded": False
        }
                
    except Exception as e:
        print(f"ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {traceback.format_exc()}")
        error_data = {
            "error": str(e),
            "sessionId": project_id,
            "type": "error"
        }
        return {
            "statusCode": 500,
            "headers": _get_sse_headers(),
            "body": f"data: {json.dumps(error_data)}\n\n",
            "isBase64Encoded": False
        }

def _handle_standard_generation(project_id, user_input, chat_history, prompt_cards, model_id):
    """ì¼ë°˜(non-streaming) Bedrock ì‘ë‹µì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    try:
        print(f"ì¼ë°˜ ìƒì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}, ëª¨ë¸={model_id}")
        final_prompt = _build_final_prompt(project_id, user_input, chat_history, prompt_cards)
        
        # ëª¨ë¸ì— ë”°ë¥¸ ìš”ì²­ ë³¸ë¬¸ êµ¬ì„±
        if model_id.startswith("anthropic."):
            request_body = {
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 4096,
                "messages": [{"role": "user", "content": final_prompt}],
                "temperature": 0.1,
                "top_p": 0.9
            }
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ë“¤ì„ ìœ„í•œ ìš”ì²­ í˜•ì‹
            request_body = {
                "prompt": final_prompt,
                "max_gen_len": 4096,
                "temperature": 0.1,
                "top_p": 0.9,
            }

        response = bedrock_client.invoke_model(
            modelId=model_id,
            body=json.dumps(request_body)
        )
        response_body = json.loads(response['body'].read())
        
        # ëª¨ë¸ë³„ ì‘ë‹µ í˜•ì‹ ì²˜ë¦¬
        if model_id.startswith("anthropic."):
            # Anthropic ëª¨ë¸ ì‘ë‹µ í˜•ì‹
            result_text = response_body['content'][0]['text']
        else:
            # Meta Llamaë‚˜ Amazon Nova ëª¨ë¸ ì‘ë‹µ í˜•ì‹
            if 'generation' in response_body:
                result_text = response_body['generation']
            elif 'outputs' in response_body:
                result_text = response_body['outputs'][0]['text']
            else:
                result_text = response_body.get('text', str(response_body))
        
        print(f"ì¼ë°˜ ìƒì„± ì™„ë£Œ: ì‘ë‹µ ê¸¸ì´={len(result_text)}")
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps({"result": result_text}),
            "isBase64Encoded": False
        }
    except Exception as e:
        print(f"ì¼ë°˜ ìƒì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        return _create_error_response(500, f"Bedrock í˜¸ì¶œ ì˜¤ë¥˜: {e}")

def _build_final_prompt(project_id, user_input, chat_history, prompt_cards):
    """í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œì™€ ì±„íŒ… íˆìŠ¤í† ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤."""
    try:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì‹œì‘: í”„ë¡œì íŠ¸ ID={project_id}")
        print(f"ì „ë‹¬ë°›ì€ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ìˆ˜: {len(prompt_cards)}")
        print(f"ì „ë‹¬ë°›ì€ ì±„íŒ… íˆìŠ¤í† ë¦¬ ìˆ˜: {len(chat_history)}")
        
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ì†¡ëœ í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì‚¬ìš© (ì´ë¯¸ í™œì„±í™”ëœ ê²ƒë§Œ í•„í„°ë§ë˜ì–´ ì „ì†¡ë¨)
        system_prompt_parts = []
        for card in prompt_cards:
            prompt_text = card.get('prompt_text', '').strip()
            if prompt_text:
                title = card.get('title', 'Untitled')
                print(f"í”„ë¡¬í”„íŠ¸ ì¹´ë“œ ì ìš©: '{title}' ({len(prompt_text)}ì)")
                system_prompt_parts.append(prompt_text)
        
        system_prompt = "\n\n".join(system_prompt_parts)
        print(f"ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(system_prompt)}ì")
        
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ êµ¬ì„±
        history_parts = []
        for msg in chat_history:
            role = msg.get('role', '')
            content = msg.get('content', '')
            if role and content:
                if role == 'user':
                    history_parts.append(f"Human: {content}")
                elif role == 'assistant':
                    history_parts.append(f"Assistant: {content}")
        
        history_str = "\n\n".join(history_parts)
        print(f"ì±„íŒ… íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(history_str)}ì")
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_parts = []
        
        # 1. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—­í• , ì§€ì¹¨ ë“±)
        if system_prompt:
            prompt_parts.append(system_prompt)
        
        # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬
        if history_str:
            prompt_parts.append(history_str)
        
        # 3. í˜„ì¬ ì‚¬ìš©ì ì…ë ¥
        prompt_parts.append(f"Human: {user_input}")
        prompt_parts.append("Assistant:")
        
        final_prompt = "\n\n".join(prompt_parts)
        print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì")
        
        return final_prompt

    except Exception as e:
        print(f"í”„ë¡¬í”„íŠ¸ êµ¬ì„± ì˜¤ë¥˜: {traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜ (íˆìŠ¤í† ë¦¬ í¬í•¨)
        try:
            history_str = "\n\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
            if history_str:
                return f"{history_str}\n\nHuman: {user_input}\n\nAssistant:"
            else:
                return f"Human: {user_input}\n\nAssistant:"
        except:
            return f"Human: {user_input}\n\nAssistant:"

def _get_sse_headers():
    """Server-Sent Events ì‘ë‹µì„ ìœ„í•œ í—¤ë”ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
        'Access-Control-Allow-Origin': '*',
        'X-Accel-Buffering': 'no'  # NGINX ë²„í¼ë§ ë¹„í™œì„±í™”
    }

def contains_recent_date_keywords(query):
    """ìµœì‹  ë‰´ìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ëŠ”ì§€ í™•ì¸"""
    recent_keywords = [
        'ì˜¤ëŠ˜', 'ì–´ì œ', 'ì´ë²ˆì£¼', 'ì´ë²ˆë‹¬', 'ìµœê·¼', 'ì§€ê¸ˆ', 'í˜„ì¬',
        'ìµœì‹ ', 'ë°©ê¸ˆ', 'ê¸ˆì¼', 'ì‹¤ì‹œê°„', 'today', 'recent'
    ]
    
    current_year = datetime.now().year
    year_keywords = [str(current_year), str(current_year - 1)]
    
    query_lower = query.lower()
    
    for keyword in recent_keywords + year_keywords:
        if keyword in query_lower:
            return True
    
    return False

def search_with_perplexity(query):
    """Perplexity AIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ê²€ìƒ‰"""
    try:
        logger.info(f"Perplexity ê²€ìƒ‰ ì‹œì‘: {query}")
        
        url = "https://api.perplexity.ai/chat/completions"
        
        payload = {
            "model": "llama-3.1-sonar-large-128k-online",
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì„œìš¸ê²½ì œì‹ ë¬¸ì˜ ë‰´ìŠ¤ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ ê²½ì œ ë‰´ìŠ¤ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì‹œê³ , ì¶œì²˜ë¥¼ ëª…ì‹œí•´ì£¼ì„¸ìš”."
                },
                {
                    "role": "user", 
                    "content": f"í•œêµ­ ê²½ì œì™€ ê´€ë ¨ëœ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {query}"
                }
            ],
            "max_tokens": 1000,
            "temperature": 0.2,
            "top_p": 0.9,
            "return_citations": True,
            "search_domain_filter": ["kr"],
            "search_recency_filter": "week"
        }
        
        headers = {
            "Authorization": f"Bearer {PERPLEXITY_API_KEY}",
            "Content-Type": "application/json"
        }
        
        logger.info("Perplexity API í˜¸ì¶œ ì¤‘...")
        
        # urllibë¥¼ ì‚¬ìš©í•œ HTTP ìš”ì²­
        req_data = json.dumps(payload).encode('utf-8')
        request = urllib.request.Request(url, data=req_data, headers=headers, method='POST')
        
        try:
            with urllib.request.urlopen(request, timeout=30) as response:
                response_data = response.read().decode('utf-8')
                data = json.loads(response_data)
                content = data['choices'][0]['message']['content']
        except urllib.error.HTTPError as e:
            logger.error(f"HTTP Error: {e.code} - {e.reason}")
            raise Exception(f"Perplexity API HTTP Error: {e.code}")
        except urllib.error.URLError as e:
            logger.error(f"URL Error: {e.reason}")
            raise Exception(f"Perplexity API Connection Error: {e.reason}")
        
        citations = []
        if 'citations' in data:
            citations = data['citations']
        
        logger.info("Perplexity ê²€ìƒ‰ ì™„ë£Œ")
        return {
            'answer': content,
            'sources': citations,
            'search_type': 'perplexity',
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Perplexity API ì˜¤ë¥˜: {str(e)}")
        # Perplexity ì‹¤íŒ¨ ì‹œ Knowledge Baseë¡œ í´ë°±
        return search_knowledge_base(query)

def search_knowledge_base(query):
    """Knowledge Baseë¥¼ ì‚¬ìš©í•œ ê³¼ê±° ë‰´ìŠ¤ ê²€ìƒ‰"""
    try:
        logger.info(f"Knowledge Base ê²€ìƒ‰ ì‹œì‘: {query}")
        
        response = bedrock_agent_client.retrieve_and_generate(
            input={'text': query},
            retrieveAndGenerateConfiguration={
                'type': 'KNOWLEDGE_BASE',
                'knowledgeBaseConfiguration': {
                    'knowledgeBaseId': KNOWLEDGE_BASE_ID,
                    'modelArn': f'arn:aws:bedrock:ap-northeast-2:887078546492:inference-profile/{DEFAULT_MODEL_ID}',
                    'retrievalConfiguration': {
                        'vectorSearchConfiguration': {
                            'numberOfResults': 10,
                            'overrideSearchType': 'HYBRID'
                        }
                    },
                    'generationConfiguration': {
                        'promptTemplate': {
                            'textPromptTemplate': '''ë‹¹ì‹ ì€ ì„œìš¸ê²½ì œì‹ ë¬¸ì˜ ë‰´ìŠ¤ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: $search_results$

ì‚¬ìš©ì ì§ˆë¬¸: $query$

ë‹µë³€ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”
2. ì •í™•í•œ ì •ë³´ë§Œ ì œê³µí•˜ê³ , í™•ì‹¤í•˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”
3. ê°€ëŠ¥í•œ í•œ êµ¬ì²´ì ì¸ ë‚ ì§œ, ìˆ˜ì¹˜, ì¶œì²˜ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”
4. ë‹µë³€ ëì— ê´€ë ¨ ì¶œì²˜ë¥¼ [1], [2] í˜•ì‹ìœ¼ë¡œ ë²ˆí˜¸ë¥¼ ë§¤ê²¨ í‘œì‹œí•˜ê³ , ê° ì¶œì²˜ì˜ URLì´ ìˆë‹¤ë©´ í•¨ê»˜ ì œê³µí•´ì£¼ì„¸ìš”
5. ì¶œì²˜ í˜•ì‹: [1] ê¸°ì‚¬ì œëª© - URL (ìˆëŠ” ê²½ìš°)

ë‹µë³€:'''
                        }
                    }
                }
            }
        )
        
        answer = response.get('output', {}).get('text', '')
        sources = []
        
        if 'citations' in response:
            for i, citation in enumerate(response['citations'], 1):
                for reference in citation.get('retrievedReferences', []):
                    source_info = {
                        'number': i,
                        'title': reference.get('metadata', {}).get('title', ''),
                        'url': reference.get('metadata', {}).get('url', ''),
                        'date': reference.get('metadata', {}).get('date', ''),
                        'category': reference.get('metadata', {}).get('category', ''),
                        'content': reference.get('content', {}).get('text', '')[:200] + '...'
                    }
                    sources.append(source_info)
        
        logger.info("Knowledge Base ê²€ìƒ‰ ì™„ë£Œ")
        return {
            'answer': answer,
            'sources': sources,
            'search_type': 'knowledge_base',
            'timestamp': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Knowledge Base ì˜¤ë¥˜: {str(e)}")
        raise

def _handle_knowledge_base_generation(project_id, user_input, chat_history, model_id):
    """
    Smart Query Routerë¥¼ í™œìš©í•œ ì¡°ê±´ë¶€ ë¶„ê¸° ê¸°ë°˜ ì§€ëŠ¥í˜• ë‰´ìŠ¤ ê²€ìƒ‰
    - ë‚ ì§œ í‘œí˜„ ê°ì§€ â†’ í•´ë‹¹ ê¸°ê°„ í•„í„°ë§ ê²€ìƒ‰
    - ì• ë§¤í•œ ì§ˆë¬¸ â†’ Perplexity API ìš°ì„  ê²€ìƒ‰  
    - ëª…í™•í•œ ì§ˆë¬¸ â†’ ì§ì ‘ ë‚´ë¶€ ê²€ìƒ‰
    - ë‚ ì§œ ì—†ìŒ â†’ ìµœì‹ ìˆœ ìš°ì„  ê²€ìƒ‰
    """
    try:
        logger.info(f"ğŸ¯ Smart Query Router ê²€ìƒ‰ ì‹œì‘: í”„ë¡œì íŠ¸={project_id}, ì§ˆë¬¸={user_input}")
        
        # Smart Query Router ì‚¬ìš© (ìµœìš°ì„ )
        if SMART_ROUTER_AVAILABLE:
            return _execute_smart_router_workflow(user_input, project_id, chat_history, model_id)
        
        # Fallback 1: Enhanced Agent System
        elif ENHANCED_AGENTS_AVAILABLE:
            logger.info("Fallback: Enhanced Agent System ì‚¬ìš©")
            return _execute_enhanced_agent_workflow(user_input, project_id, chat_history, model_id)
        
        # Fallback 2: ê¸°ì¡´ ì‹œìŠ¤í…œ
        else:
            logger.info("Fallback: ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰")
            return _execute_legacy_workflow(user_input, project_id, chat_history, model_id)
        
    except Exception as e:
        logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {traceback.format_exc()}")
        return _create_error_response(500, f"ë‰´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


def _execute_smart_router_workflow(user_input, project_id, chat_history, model_id):
    """
    Smart Query Router ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    """
    try:
        logger.info("ğŸ¯ SmartQueryRouter ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        
        # 1. SmartQueryRouter ì´ˆê¸°í™”
        logger.info("ğŸ” SmartQueryRouter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œë„ ì¤‘...")
        try:
            smart_router = SmartQueryRouter()
            logger.info("âœ… SmartQueryRouter ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì„±ê³µ!")
        except Exception as router_error:
            logger.error(f"âŒ SmartQueryRouter ìƒì„± ì‹¤íŒ¨: {str(router_error)}")
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise router_error
        
        # 2. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        logger.info("ğŸ” ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì¤‘...")
        user_context = {
            "project_id": project_id,
            "chat_history": chat_history,
            "model_id": model_id,
            "timestamp": datetime.now().isoformat()
        }
        logger.info(f"âœ… ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ: {list(user_context.keys())}")
        
        logger.info("ğŸš€ Smart Query Router ì‹¤í–‰...")
        
        # 3. ë©”ì¸ ë¼ìš°íŒ… ë° ì‹¤í–‰
        try:
            router_result = smart_router.route_and_execute(
                query=user_input,
                context=user_context
            )
            logger.info(f"âœ… Smart Query Router ì‹¤í–‰ ì™„ë£Œ: {router_result.get('success', False)}")
        except Exception as execute_error:
            logger.error(f"âŒ Smart Query Router ì‹¤í–‰ ì‹¤íŒ¨: {str(execute_error)}")
            logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            raise execute_error
        
        # 4. ê²°ê³¼ ì²˜ë¦¬
        if router_result["success"]:
            final_result = router_result["result"]
            answer = final_result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sources = final_result.get("sources", [])
            metadata = final_result.get("metadata", {})
            
            logger.info(f"âœ… ë‹µë³€ ìƒì„± ì„±ê³µ: {len(answer)}ì, ì¶œì²˜: {len(sources)}ê°œ")
            
            # 5. Smart Router ì „ìš© ì‘ë‹µ í¬ë§·íŒ…
            try:
                formatted_response = _format_smart_router_response(
                    answer, sources, metadata, router_result
                )
                logger.info("âœ… ì‘ë‹µ í¬ë§·íŒ… ì™„ë£Œ")
            except Exception as format_error:
                logger.error(f"âŒ ì‘ë‹µ í¬ë§·íŒ… ì‹¤íŒ¨: {str(format_error)}")
                formatted_response = answer  # ê¸°ë³¸ ë‹µë³€ ì‚¬ìš©
            
            return {
                "statusCode": 200,
                "headers": {
                    "Content-Type": "application/json",
                    "Access-Control-Allow-Origin": "*"
                },
                "body": json.dumps({"result": formatted_response}),
                "isBase64Encoded": False
            }
        else:
            error_msg = router_result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
            logger.error(f"âŒ SmartQueryRouter ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
            return _create_error_response(500, f"Smart Router ì˜¤ë¥˜: {error_msg}")
            
    except Exception as e:
        logger.error(f"âŒ SmartQueryRouter ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")
        logger.error(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return _create_error_response(500, f"Smart Router ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")


def _execute_enhanced_agent_workflow(user_input, project_id, chat_history, model_id):
    """
    ìƒˆë¡œìš´ ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    """
    try:
        # 1. ConditionalExecutionEngine ì´ˆê¸°í™”
        execution_engine = ConditionalExecutionEngine()
        
        # 2. ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        user_context = {
            "project_id": project_id,
            "chat_history": chat_history,
            "model_id": model_id,
            "timestamp": datetime.now().isoformat()
        }
        
        # 3. ì‹¤í–‰ ì˜µì…˜ ì„¤ì •
        execution_options = {
            "enable_external_search": bool(PERPLEXITY_API_KEY),
            "enable_date_intelligence": True,
            "enable_few_shot": True,
            "debug": True
        }
        
        logger.info("Enhanced Agent ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘...")
        
        # 4. ë©”ì¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        workflow_result = execution_engine.execute_workflow(
            query=user_input,
            user_context=user_context,
            execution_options=execution_options
        )
        
        if workflow_result["success"]:
            # ì„±ê³µì ì¸ ê²°ê³¼ ì²˜ë¦¬
            final_result = workflow_result["result"]
            answer = final_result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            sources = final_result.get("sources", [])
            metadata = final_result.get("metadata", {})
            
            # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_response = _format_enhanced_response(answer, sources, metadata, workflow_result)
            
            logger.info(f"Enhanced Agent ì™„ë£Œ - ì‹¤í–‰ì‹œê°„: {workflow_result.get('metadata', {}).get('execution_time', 0):.2f}s")
            
            return {
                "statusCode": 200,
                "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
                "body": json.dumps({"result": formatted_response}),
                "isBase64Encoded": False
            }
        else:
            # ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì‘ë‹µ
            error_msg = workflow_result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
            logger.error(f"Enhanced Agent ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
            return _create_error_response(500, f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {error_msg}")
        
    except Exception as e:
        logger.error(f"Enhanced Agent ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")
        # Fallback to legacy system
        return _execute_legacy_workflow(user_input, project_id, chat_history, model_id)


def _execute_legacy_workflow(user_input, project_id, chat_history, model_id):
    """
    ê¸°ì¡´ ì‹œìŠ¤í…œ Fallback ì›Œí¬í”Œë¡œìš°
    """
    try:
        logger.info("Legacy í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰")
        
        # ê¸°ì¡´ ë¡œì§ ìœ ì§€
        should_use_perplexity = contains_recent_date_keywords(user_input)
        
        if should_use_perplexity and PERPLEXITY_API_KEY:
            logger.info("Perplexity AIë¥¼ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ê²€ìƒ‰ ì‹¤í–‰")
            search_result = search_with_perplexity(user_input)
        else:
            logger.info("Knowledge Baseë¥¼ ì‚¬ìš©í•œ ê³¼ê±° ë°ì´í„° ê²€ìƒ‰ ì‹¤í–‰")
            search_result = search_knowledge_base(user_input)
        
        # ì‘ë‹µ í¬ë§· í†µì¼
        final_response = search_result['answer']
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        if search_result.get('sources'):
            final_response += "\n\n**ì¶œì²˜:**\n"
            for i, source in enumerate(search_result['sources'][:5], 1):
                if isinstance(source, dict):
                    title = source.get('title', '')
                    url = source.get('url', '')
                    if title and url:
                        final_response += f"{i}. {title} - {url}\n"
                    elif url:
                        final_response += f"{i}. {url}\n"
                elif isinstance(source, str):
                    final_response += f"{i}. {source}\n"
        
        final_response += f"\n*ê²€ìƒ‰ ë°©ì‹: {search_result['search_type']} (Legacy)*"
        final_response += f"\n*ê²€ìƒ‰ ì‹œê°„: {search_result['timestamp']}*"
        
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
            "body": json.dumps({"result": final_response}),
            "isBase64Encoded": False
        }
        
    except Exception as e:
        logger.error(f"Legacy ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}")
        return _create_error_response(500, f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


def _format_smart_router_response(answer, sources, metadata, router_result):
    """
    Smart Query Router ì „ìš© ì‘ë‹µ í¬ë§·íŒ…
    """
    try:
        formatted_answer = answer
        
        # ğŸ¯ ë¼ìš°íŒ… ì •ë³´ ì¶”ê°€ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
        routing_info = router_result.get("routing_info", {})
        if routing_info:
            route_type_names = {
                "date_filtered_search": "ğŸ“… ë‚ ì§œ í•„í„°ë§ ê²€ìƒ‰",
                "clarity_enhancement_flow": "â“ ëª…í™•ì„± í–¥ìƒ í”Œë¡œìš° (Perplexity ìš°ì„ )",
                "direct_internal_search": "ğŸ“š ì§ì ‘ ë‚´ë¶€ ê²€ìƒ‰ (ìµœì‹ ìˆœ ìš°ì„ )"
            }
            route_name = route_type_names.get(routing_info.get("route_type"), routing_info.get("route_type"))
            formatted_answer += f"\n\n## ğŸ¯ ì²˜ë¦¬ ë°©ì‹\n"
            formatted_answer += f"**{route_name}**\n"
            formatted_answer += f"â”” {routing_info.get('reason', 'ì¡°ê±´ë¶€ ë¶„ê¸° ì²˜ë¦¬')}\n"
        
        # ğŸ§  ì‚¬ê³  ê³¼ì • ì¶”ê°€ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
        thinking_process = router_result.get("thinking_process", [])
        if thinking_process:
            formatted_answer += "\n\n## ğŸ§  AI ì‚¬ê³  ê³¼ì •\n"
            formatted_answer += "---\n"
            for i, step in enumerate(thinking_process[:6], 1):
                step_name = step.get("step_name", f"ë‹¨ê³„ {i}")
                step_description = step.get("description", "")
                step_result = step.get("result", "")
                execution_time = step.get("execution_time", 0)
                
                formatted_answer += f"**{i}. {step_name}** ({execution_time:.1f}ì´ˆ)\n"
                if step_description:
                    formatted_answer += f"   â”” {step_description}\n"
                if step_result and len(step_result) < 100:
                    formatted_answer += f"   â†’ {step_result}\n"
                formatted_answer += "\n"
        
        # ğŸ“š ì¶œì²˜ ì •ë³´ ì¶”ê°€
        if sources:
            formatted_answer += "\n## ğŸ“š ì¶œì²˜\n"
            formatted_answer += "---\n"
            for i, source in enumerate(sources[:5], 1):
                title = source.get("title", "ì œëª© ì—†ìŒ")[:60]
                url = source.get("url", "")
                content_preview = source.get("content", "")[:100]
                
                formatted_answer += f"**[{i}] {title}**\n"
                if content_preview:
                    formatted_answer += f"   â”” {content_preview}...\n"
                if url:
                    formatted_answer += f"   ğŸ”— {url}\n"
                formatted_answer += "\n"
        
        # âš™ï¸ ì‹¤í–‰ ì •ë³´ ì¶”ê°€
        execution_time = router_result.get("execution_time", 0)
        if execution_time > 0:
            formatted_answer += f"\n## âš™ï¸ ì‹¤í–‰ ì •ë³´\n"
            formatted_answer += f"**ì´ ì²˜ë¦¬ ì‹œê°„**: {execution_time:.1f}ì´ˆ\n"
            formatted_answer += f"**ê²€ìƒ‰ëœ ì†ŒìŠ¤**: {len(sources)}ê°œ\n"
            if routing_info.get("route_type"):
                formatted_answer += f"**ë¼ìš°íŒ… íƒ€ì…**: {routing_info['route_type']}\n"
        
        return formatted_answer
        
    except Exception as e:
        logger.error(f"Smart Router ì‘ë‹µ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
        return answer or "ë‹µë³€ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


def _format_enhanced_response(answer, sources, metadata, workflow_result):
    """
    Enhanced Agent ê²°ê³¼ë¥¼ í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    """
    try:
        formatted_answer = answer
        
        # ì‚¬ê³  ê³¼ì • ì¶”ê°€ (ìƒˆë¡œìš´ ê¸°ëŠ¥!)
        thinking_process = workflow_result.get("thinking_process", [])
        if thinking_process:
            formatted_answer += "\n\n## AI ì‚¬ê³  ê³¼ì •\n"
            formatted_answer += "---\n"
            for i, step in enumerate(thinking_process[:5], 1):  # ìµœëŒ€ 5ë‹¨ê³„
                step_name = step.get("step_name", f"ë‹¨ê³„ {i}")
                step_description = step.get("description", "")
                step_result = step.get("result", "")
                execution_time = step.get("execution_time", 0)
                
                formatted_answer += f"**{i}. {step_name}** ({execution_time:.1f}ì´ˆ)\n"
                if step_description:
                    formatted_answer += f"   â”” {step_description}\n"
                if step_result and len(step_result) < 100:  # ì§§ì€ ê²°ê³¼ë§Œ í‘œì‹œ
                    formatted_answer += f"   â†’ {step_result}\n"
                formatted_answer += "\n"
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (ê¸°ì¡´ í˜•ì‹ ìœ ì§€)
        if sources:
            formatted_answer += "\n## ì¶œì²˜\n"
            formatted_answer += "---\n"
            for source in sources[:5]:  # ìµœëŒ€ 5ê°œ
                citation_num = source.get("citation_number", 1)
                title = source.get("title", "")
                url = source.get("url", "")
                source_type = source.get("type", "unknown")
                
                if title and url:
                    formatted_answer += f"[{citation_num}] {title} - {url}\n"
                elif url:
                    formatted_answer += f"[{citation_num}] {url}\n"
                elif title:
                    formatted_answer += f"[{citation_num}] {title} ({source_type})\n"
        
        # ë©”íƒ€ë°ì´í„° ì¶”ê°€ (í–¥ìƒëœ í˜•ì‹)
        execution_metadata = workflow_result.get("metadata", {})
        
        formatted_answer += f"\n\n##  ì‹¤í–‰ ì •ë³´\n"
        formatted_answer += f"---\n"
        formatted_answer += f" **ê²€ìƒ‰ ë°©ì‹:** Enhanced Agent System\n"
        formatted_answer += f" **ì‹¤í–‰ ì‹œê°„:** {execution_metadata.get('execution_time', 0):.2f}ì´ˆ\n"
        formatted_answer += f" **ì‹¤í–‰ ë‹¨ê³„:** {execution_metadata.get('steps_executed', 0)}ê°œ\n"
        
        if execution_metadata.get('external_search_used'):
            formatted_answer += f"ğŸŒ **ì™¸ë¶€ ê²€ìƒ‰:** Perplexity API ì‚¬ìš©\n"
        
        formatted_answer += f" **í’ˆì§ˆ ì ìˆ˜:** {metadata.get('quality_score', 0):.2f}/5.0\n"
        formatted_answer += f" **ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        
        return formatted_answer
        
    except Exception as e:
        logger.error(f"ì‘ë‹µ í¬ë§·íŒ… ì˜¤ë¥˜: {str(e)}")
        return f"{answer}\n\n*Enhanced Agent System (í¬ë§·íŒ… ì˜¤ë¥˜)*"

def _create_error_response(status_code, message):
    """ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    return {
        "statusCode": status_code,
        "headers": {"Content-Type": "application/json", "Access-Control-Allow-Origin": "*"},
        "body": json.dumps({"error": message, "timestamp": datetime.utcnow().isoformat()}),
        "isBase64Encoded": False
        } 